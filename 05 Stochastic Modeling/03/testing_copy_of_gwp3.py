# -*- coding: utf-8 -*-
"""TESTING copy of GWP3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wxfdoa6dtT8YMBMoZne5uZ6V7RxBOwYZ

# STEP 2: Pseudocode of the multi-armed bandit problem

Define δ

Define N (number of trials)

Download historical data for the prices of a list of stocks that we are interested in

Turn the prices into returns.

Estimate the correlation structure and risk level.

Filter the list of the stocks to select a basket of K assets

For 1 to N do:

Choose portfolio of stocks and give them weights.

Find the returns that this portfolio gives.
"""



"""# STEP 3: Collect data

## Financial companies
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
import yfinance as yf
import seaborn as sns
from numpy.random import rand, seed

"""Choose tickers"""

tickers_fin = ["JPM","WFC","BAC", "C", "GS", "USB", "MS", "KEY", "PNC", "COF", "AXP",
           "PRU", "SCHW"]

"""Collect data for the period Sept and Oct 2008"""

df_fin = pd.DataFrame()
for i in tickers_fin:
  ydata = yf.download(i, start = '2008-09-01', end = '2008-11-01')
  df_fin[i] = ydata['Adj Close']
df_fin.index = pd.to_datetime(ydata.index, format='%Y%m%d')
df_fin.head()

"""## Non-financial companies"""

tickers_nfin = ["KR", "PFE", "XOM", "WMT", "DAL", "CSCO", "EQIX", "DUK",
                "NFLX", "GE", "APA", "F", "REGN", "CMS"]

df_nfin = pd.DataFrame()
for i in tickers_nfin:
  ydata = yf.download(i, start = '2008-09-01', end = '2008-11-01')
  df_nfin[i] = ydata['Adj Close']
df_nfin.index = pd.to_datetime(ydata.index, format='%Y%m%d')
df_nfin.head()

"""Couldn't get data for non financial tickers("HCP"), and from financial tickers("BBT", "STI").

## Compute financial returns

Financial Companies
"""

df_fin_returns = df_fin.pct_change(axis=0) # daily returns
df_fin_returns = df_fin_returns.dropna()
df_fin_returns

plt.plot(df_fin_returns.index,df_fin_returns)
plt.legend(tickers_fin, loc='upper left')
plt.xlabel("Date")
plt.ylabel("Percent")
plt.title("Daily returns data of Financial Companies")
plt.xticks(rotation=90)
plt.show()

"""Non-financial companies"""

df_nfin_returns = df_nfin.pct_change(axis=0) # daily returns
df_nfin_returns = df_nfin_returns.dropna()
df_nfin_returns

plt.plot(df_nfin_returns.index,df_nfin_returns)
plt.legend(tickers_nfin, loc='upper left')
plt.xlabel("Date")
plt.ylabel("Percent")
plt.title("Daily returns data of Non-Financial Companies")
plt.xticks(rotation=90)
plt.show()

"""# Step 4: Correlation matrix

We combine the two list of returns:
"""

df_returns = pd.DataFrame()

df_returns = pd.concat([df_nfin_returns, df_fin_returns],axis=1)

df_returns

zero_values = (df_returns == 0).sum().sum()

if zero_values > 0:
    print(f"There are {zero_values} zero values in the dataset.")
else:
    print("There are no zero values in the dataset.")

df_returns = df_returns.replace(0, 0.01)

zero_values = (df_returns == 0).sum().sum()

if zero_values > 0:
    print(f"There are {zero_values} zero values in the dataset.")
else:
    print("There are no zero values in the dataset.")

plt.plot(df_returns.index,df_returns)
plt.xlabel("Date")
plt.ylabel("Percent")
plt.title("Daily returns between Sept-Oct 2008")
plt.xticks(rotation=90)
plt.show()

"""Compute the correlation matrix"""

correlation_matrix = df_returns.corr()

correlation_matrix

"""Heatmap of the correlation matrix

1)
"""

sns.heatmap(correlation_matrix)

"""2)"""

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.show()

"""Sort the securities:

We first calculate the average correlation for each security
"""

maximum_corr = []

for i in correlation_matrix.columns:
  max_corr = correlation_matrix[i].drop(i).max()
  maximum_corr.append([i,f'{max_corr*100:.2f}%'])

maximum_corr = pd.DataFrame(maximum_corr,columns=['Tickers', 'Max Correlation'])

maximum_corr

"""Then we sort the securities based on average correlation"""

sort = maximum_corr.sort_values(by='Max Correlation', ascending=False)

sort

"""Create two list for the high-correlated, and not correlated stocks.

First, we would set a threshold over the mean:
"""

maximum_corr['Max Correlation'].str.rstrip('%').astype(float)

maximum_corr['Numerical correlation'] = maximum_corr['Max Correlation'].str.rstrip('%').astype(float)
maximum_corr

threshold = maximum_corr['Numerical correlation'].mean()
threshold

"""Then, we create the two lists, for high correlated, and low correlated:"""

high_corr_assets = maximum_corr[maximum_corr['Numerical correlation'] > threshold]['Tickers'].tolist()
low_corr_assets = maximum_corr[maximum_corr['Numerical correlation'] <= threshold]['Tickers'].tolist()

print("high_corr_assets",high_corr_assets)
print("low_corr_assets",low_corr_assets)

"""# Step 6: UBC algorithm

### a) pseudocode

Initialize:

N(a) = 0 for all a

Q(a) = 0 for all a

For each round:

For each arm a:

If N(a) > 0:

  UCB(a) = Q(a) + sqrt((2 * log(total count of rounds)) / N(a))

Else:

  UCB(a) = Infinity
  
a_max = argmax_a UCB(a) # Choose the arm which has maximum UCB

Reward = pullBandit(a_max) # Pull the chosen arm and get the reward

N(a_max) = N(a_max) + 1 # Increment the count of chosen arm

Q(a_max) = Q(a_max) + (Reward - Q(a_max)) / N(a_max) # Update the estimated value of chosen arm

In this pseudocode:

• N(a) is the number of times action a has been selected.

• Q(a) is the estimated value of action a.

• UCB(a) is the upper confidence bound of action a.

• pullBandit(a) is a function to pull arm a of the bandit and it returns a reward.

## b) Python implementation
"""

# Read stock price information

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from numpy.random import rand, seed


def optimal_action(qvalue, eps):  # noQA E203
    """
    Determines what is the action to take given a measure of past
    expected rewards across actions. With probability eps the action
    is not the greedy one
    """
    nactions = qvalue.shape[0]
    action_hat = np.where(qvalue == np.max(qvalue))

    if rand() <= eps:
        randnum = rand()
        for aa in range(nactions):
            if randnum < (aa + 1) / nactions:  # noQA E203
                break
    elif action_hat[0].shape[0] > 1:  # noQA E203
        # Randomize action when ties
        randnum = rand()
        for aa in range(action_hat[0].shape[0]):  # noQA E203
            if randnum < (aa + 1) / action_hat[0].shape[0]:  # noQA E203
                break
        aa = action_hat[0][aa]
    else:
        aa = np.argmax(qvalue)

    return aa


def reward_update(action, reward, qvalue_old, alpha):  # noQA E203
    qvalue_new = qvalue_old.copy()

    qvalue_new[action] = qvalue_old[action] + alpha * (reward - qvalue_old[action])

    return qvalue_new

pdata = df_returns.to_numpy()
pdata_dates = pd.to_datetime(df_returns.index, format='%Y-%m-%d')


#	Bandit problem for stock selection
NK = pdata.shape[1]

EPSILON = 0.0    # e-greedy setting TURNED OFF
ALPHA =0.9
NEPISODES = 1000
HOLD = 1
TMAX = pdata.shape[0] - HOLD

#	NEW PARAMETER
UCB_WEIGHT =1.0    # UCB setting TURNED ON
seed(1234)
reward_avg = np.zeros((TMAX))
optimal_avg = np.zeros((TMAX))
reward_queue = np.zeros((HOLD,2))

for run in range(NEPISODES):
# Initialize q function and actions record
  qvalue = np.zeros((NK))
  qvalue_up = np.zeros((NK))
  nactions = np.zeros((NK))

  for tt in range(TMAX):
    aa_opt = optimal_action(qvalue_up,EPSILON)
    nactions[aa_opt] += 1

  #	Compute reward as return over holding period
    reward_queue[HOLD-1,0] = (pdata[tt+HOLD,aa_opt]-pdata[tt,aa_opt])/pdata[tt,aa_opt]
    reward_queue[HOLD-1,1] = aa_opt

#	Update Q function using action chosen HOLD days before

    qvalue = reward_update(int(reward_queue[0,1]), reward_queue[0,0], qvalue, ALPHA)

    #qvalue = reward_update(int(reward_queue[0,1]), reward_queue[0,0], qvalue, 1/nactions[aa_opt])
    #print(qvalue)

   #	Upper-confidence adjustment

    qvalue_up = np.zeros((NK))
    for aa in range(NK):
      # If an action has not been visited simply give it the maximum value across actions
      if nactions[aa] == 0:
        qvalue_up[aa] = np.max(qvalue_up) +1.0
      else:
        qvalue_up[aa] = qvalue[aa] + UCB_WEIGHT * np.sqrt(np.log(tt+1)/nactions[aa])

    reward_queue[0:HOLD-1,:] = reward_queue[1:HOLD,:]
    reward_avg[tt] += reward_queue[HOLD-1,0]/NEPISODES
    optimal_avg[tt] += (aa_opt==np.argmax((pdata[tt+HOLD,:]-pdata[tt,:])/pdata[tt,:]))/NEPISODES

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],optimal_avg)

plt.title("Frequency of optimal action UCB", fontsize='x-large')
plt.xticks(rotation=90)
fig = plt.gcf()

fig.set_size_inches(12, 3)

plt.show()

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],np.max((pdata[HOLD:pdata.shape[0],:]-pdata[0:TMAX,:])/pdata [0:TMAX,:] , axis=1), label='Max returns')

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],reward_avg, label='Average reward')
plt.xticks(rotation=90)
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
fig.set_size_inches(12, 3)
plt.show()

plt.plot(pdata_dates[HOLD:pdata.shape[0] ],np.mean((pdata[HOLD:pdata.shape[0],:]-pdata[0:TMAX,:])/pdata[0:TMAX,:], axis=1), label='Average returns')
plt.plot(pdata_dates[HOLD:pdata.shape[0]],reward_avg, label='Average reward')
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
plt.xticks(rotation=90)
fig.set_size_inches(12, 3)
plt.show()

#	Average frequency of optimal action
print("Optimal Rewards",np.mean(optimal_avg))

#	Average reward across steps
print("Average Rewards",np.mean(reward_avg))

#	Average annualized return from holding the equally-weighted portfolio
print("Average annualized return from holding the equally-weighted portfolio",(1 + np.mean((pdata[HOLD:pdata.shape[0] ,:]-pdata[0:TMAX,:])/pdata[0:TMAX,:])) ** (250 / HOLD) - 1,np.sqrt(250 / HOLD) * np.std(np.mean((pdata))))

#	Average annualized return from holding the Bandit portfolio
print("Average annualized return from holding the Bandit portfolio",(1+np.mean(reward_avg)) ** (250 / HOLD) - 1 , np.sqrt(250 / HOLD) * np.std(reward_avg))

return_cumulative = np.zeros((TMAX+1,2))
return_cumulative[0,0] = 1
return_cumulative[0,1] = 1
for tt in range(1,TMAX+1):
  return_cumulative[tt,0] = return_cumulative[tt-1,0] * ( 1 + reward_avg[tt-1] )
  rmean = np.mean((pdata[tt+HOLD-1,:]-pdata[tt-1,:])/pdata[tt-1,:])
  return_cumulative[tt,1] = return_cumulative[tt-1,1] * ( 1 + rmean )

plt.plot(pdata_dates[HOLD-1:pdata.shape[0]],return_cumulative[:,0], label='Cumulative return Bandit')
plt.plot(pdata_dates[HOLD-1:pdata.shape[0]],return_cumulative[:,1], label='Cumulative return Average')
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
fig.set_size_inches(12, 3)
plt.xticks(rotation=90)
plt.show()

"""# Step 8: Epsilon-greedy algorithm

## a) Pseudocode

First step of algorithm: Create a function to obtain the optimal action choice
depending on a measure of past expected rewards.


  if rand() <= eps:

    random action

  else

    greedy action


Second step of Algorithm: Create a function to update the expected reward
  Set the new qvalue equal to the old qvalue + a variance 'alpha'


Third step: Estimate the Algorithm over a number of steps

1) Create a for loop over the number of steps

2) Initialize the parameters

3) Use the optimal action algorithm to determine the type of action (random or greedy).

4) Use the expected reward algorithm based on the choice that comes from the
previous function.

5) Calculate the average and optimal rewards across episodes

## b) Python implementation

We are reusing the UCB code but this time we are defining the variable UCB_WEIGHT =0.0 so that we turn off the UCB algorithm and turn it into e-greedy algorithm

Set the initial parameters
"""

pdata = df_returns.to_numpy()
pdata_dates = pd.to_datetime(df_returns.index, format='%Y-%m-%d')


#	Bandit problem for stock selection
NK = pdata.shape[1]

EPSILON = 0.4    # e-greedy setting TURNED ON
ALPHA =0.9
NEPISODES = 1000
HOLD = 1
TMAX = pdata.shape[0] - HOLD

#	NEW PARAMETER
UCB_WEIGHT =0.0   # UCB turned off
seed(1234)
reward_avg = np.zeros((TMAX))
optimal_avg = np.zeros((TMAX))
reward_queue = np.zeros((HOLD,2))

for run in range(NEPISODES):
# Initialize q function and actions record
  qvalue = np.zeros((NK))
  qvalue_up = np.zeros((NK))
  nactions = np.zeros((NK))

  for tt in range(TMAX):
    aa_opt = optimal_action(qvalue_up,EPSILON)
    nactions[aa_opt] += 1

  #	Compute reward as return over holding period
    reward_queue[HOLD-1,0] = (pdata[tt+HOLD,aa_opt]-pdata[tt,aa_opt])/pdata[tt,aa_opt]
    reward_queue[HOLD-1,1] = aa_opt

#	Update Q function using action chosen HOLD days before

    #qvalue = reward_update(int(reward_queue[0,1]), reward_queue[0,0], qvalue, ALPHA)

    qvalue = reward_update(int(reward_queue[0,1]), reward_queue[0,0], qvalue, 1/nactions[aa_opt])
    #print(qvalue)

   #	Upper-confidence adjustment

    qvalue_up = np.zeros((NK))
    for aa in range(NK):
      # If an action has not been visited simply give it the maximum value across actions
      if nactions[aa] == 0:
        qvalue_up[aa] = np.max(qvalue_up) +1.0
      else:
        qvalue_up[aa] = qvalue[aa] + UCB_WEIGHT * np.sqrt(np.log(tt+1)/nactions[aa])

    reward_queue[0:HOLD-1,:] = reward_queue[1:HOLD,:]
    reward_avg[tt] += reward_queue[HOLD-1,0]/NEPISODES
    optimal_avg[tt] += (aa_opt==np.argmax((pdata[tt+HOLD,:]-pdata[tt,:])/pdata[tt,:]))/NEPISODES

"""Visualize the results"""

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],optimal_avg)

plt.title("Frequency of optimal action epsilon-greedy", fontsize='x-large')
plt.xticks(rotation=90)
fig = plt.gcf()

fig.set_size_inches(12, 3)

plt.show()

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],np.max((pdata[HOLD:pdata.shape[0],:]-pdata[0:TMAX,:])/pdata [0:TMAX,:] , axis=1), label='Max returns')

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],reward_avg, label='Average reward')
plt.xticks(rotation=90)
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
fig.set_size_inches(12, 3)
plt.show()

plt.plot(pdata_dates[HOLD:pdata.shape[0] ],np.mean((pdata[HOLD:pdata.shape[0],:]-pdata[0:TMAX,:])/pdata[0:TMAX,:], axis=1), label='Average returns')
plt.plot(pdata_dates[HOLD:pdata.shape[0]],reward_avg, label='Average reward')
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
plt.xticks(rotation=90)
fig.set_size_inches(12, 3)
plt.show()

#	Average frequency of optimal action
print("optimal action",np.mean(optimal_avg))

#	Average reward across steps
print("Average reward ",np.mean(reward_avg))

#	Average annualized return from holding the equally-weighted portfolio
print("Average annualized return from holding the equally-weighted portfolio",(1 + np.mean((pdata[HOLD:pdata.shape[0] ,:]-pdata[0:TMAX,:])/pdata[0:TMAX,:])) ** (250 / HOLD) - 1,np.sqrt(250 / HOLD) * np.std(np.mean((pdata))))

#	Average annualized return from holding the Bandit portfolio
print("Average annualized return from holding the Bandit portfolio",(1+np.mean(reward_avg)) ** (250 / HOLD) - 1 , np.sqrt(250 / HOLD) * np.std(reward_avg))

return_cumulative = np.zeros((TMAX+1,2))
return_cumulative[0,0] = 1
return_cumulative[0,1] = 1
for tt in range(1,TMAX+1):
  return_cumulative[tt,0] = return_cumulative[tt-1,0] * ( 1 + reward_avg[tt-1] )
  rmean = np.mean((pdata[tt+HOLD-1,:]-pdata[tt-1,:])/pdata[tt-1,:])
  return_cumulative[tt,1] = return_cumulative[tt-1,1] * ( 1 + rmean )

plt.plot(pdata_dates[HOLD-1:pdata.shape[0]],return_cumulative[:,0], label='Cumulative return Bandit')
plt.plot(pdata_dates[HOLD-1:pdata.shape[0]],return_cumulative[:,1], label='Cumulative return Average')
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
fig.set_size_inches(12, 3)
plt.xticks(rotation=90)
plt.show()

"""# Step 10: Update data

## Import 15 financial companies

Choose tickers
"""

tickers_fin_b = ["JPM","WFC","BAC", "C", "GS", "USB", "MS", "KEY", "PNC", "COF", "AXP",
           "PRU", "SCHW"]

"""Collect data using a data frame for the period between Mar and Apr 2020"""

df_fin_b = pd.DataFrame()
for i in tickers_fin:
  ydata = yf.download(i, start = '2020-03-01', end = '2020-05-01')
  df_fin_b[i] = ydata['Adj Close']
#df_fin.index = pd.to_datetime(ydata.index, format='%Y-%m-%d')
df_fin_b.head()

"""## Import 15 non-financial companies"""

tickers_nfin_b = ["KR", "PFE", "XOM", "WMT", "DAL", "CSCO", "EQIX", "DUK",
                "NFLX", "GE", "APA", "F", "REGN", "CMS"]

df_nfin_b = pd.DataFrame()
for i in tickers_nfin_b:
  ydata = yf.download(i, start = '2020-03-01', end = '2020-05-01')
  df_nfin_b[i] = ydata['Adj Close']
#df_nfin.index = pd.to_datetime(ydata.index, format='%Y%m%d')
df_nfin_b.head()



"""## Merge into a single data-structure and compute the returns

We combine the two list of time series:
"""

df_b = pd.DataFrame()

df_b = pd.concat([df_nfin_b, df_fin_b],axis=1)
df_b.head()

"""Comput the returns"""

df_b_returns = df_b.pct_change(axis=0) # daily returns
df_b_returns = df_b_returns.dropna()
df_b_returns.head()

zero_values = (df_b_returns == 0).sum().sum()

if zero_values > 0:
    print(f"There are {zero_values} zero values in the dataset.")
else:
    print("There are no zero values in the dataset.")

df_b_returns = df_b_returns.replace(0, 0.01)


zero_values = (df_b_returns == 0).sum().sum()

if zero_values > 0:
    print(f"There are {zero_values} zero values in the dataset.")
else:
    print("There are no zero values in the dataset.")

plt.plot(df_b_returns.index,df_b_returns)
plt.xlabel("Date")
plt.ylabel("Percent")
plt.title("Daily returns data between March-April 2020")
plt.xticks(rotation=90)
plt.show()

"""# Step 11

### a) UCB algorithm
"""

pdata = df_b_returns.to_numpy()
pdata_dates = pd.to_datetime(df_b_returns.index, format='%Y-%m-%d')


#	Bandit problem for stock selection
NK = pdata.shape[1]

EPSILON = 0.0    # e-greedy setting TURNED OFF
ALPHA =0.9
NEPISODES = 1000
HOLD = 5
TMAX = pdata.shape[0] - HOLD

#	NEW PARAMETER
UCB_WEIGHT =2.0    # UCB setting TURNED ON
seed(1234)
reward_avg = np.zeros((TMAX))
optimal_avg = np.zeros((TMAX))
reward_queue = np.zeros((HOLD,2))

for run in range(NEPISODES):
# Initialize q function and actions record
  qvalue = np.zeros((NK))
  qvalue_up = np.zeros((NK))
  nactions = np.zeros((NK))

  for tt in range(TMAX):
    aa_opt = optimal_action(qvalue_up,EPSILON)
    nactions[aa_opt] += 1

  #	Compute reward as return over holding period
    reward_queue[HOLD-1,0] = (pdata[tt+HOLD,aa_opt]-pdata[tt,aa_opt])/pdata[tt,aa_opt]
    reward_queue[HOLD-1,1] = aa_opt

#	Update Q function using action chosen HOLD days before

    qvalue = reward_update(int(reward_queue[0,1]), reward_queue[0,0], qvalue, ALPHA)

    #qvalue = reward_update(int(reward_queue[0,1]), reward_queue[0,0], qvalue, 1/nactions[aa_opt])
    #print(qvalue)

   #	Upper-confidence adjustment

    qvalue_up = np.zeros((NK))
    for aa in range(NK):
      # If an action has not been visited simply give it the maximum value across actions
      if nactions[aa] == 0:
        qvalue_up[aa] = np.max(qvalue_up) +1.0
      else:
        qvalue_up[aa] = qvalue[aa] + UCB_WEIGHT * np.sqrt(np.log(tt+1)/nactions[aa])

    reward_queue[0:HOLD-1,:] = reward_queue[1:HOLD,:]
    reward_avg[tt] += reward_queue[HOLD-1,0]/NEPISODES
    optimal_avg[tt] += (aa_opt==np.argmax((pdata[tt+HOLD,:]-pdata[tt,:])/pdata[tt,:]))/NEPISODES

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],optimal_avg)

plt.title("Frequency of optimal action UCB", fontsize='x-large')
plt.xticks(rotation=90)
fig = plt.gcf()

fig.set_size_inches(12, 3)

plt.show()

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],np.max((pdata[HOLD:pdata.shape[0],:]-pdata[0:TMAX,:])/pdata [0:TMAX,:] , axis=1), label='Max returns')

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],reward_avg, label='Average reward')
plt.xticks(rotation=90)
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
fig.set_size_inches(12, 3)
plt.show()

plt.plot(pdata_dates[HOLD:pdata.shape[0] ],np.mean((pdata[HOLD:pdata.shape[0],:]-pdata[0:TMAX,:])/pdata[0:TMAX,:], axis=1), label='Average returns')
plt.plot(pdata_dates[HOLD:pdata.shape[0]],reward_avg, label='Average reward')
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
plt.xticks(rotation=90)
fig.set_size_inches(12, 3)
plt.show()

#	Average frequency of optimal action
print(np.mean(optimal_avg))

#	Average annualized return from holding the equally-weighted portfolio
print((1 + np.mean((pdata[HOLD:pdata.shape[0] ,:]-pdata[0:TMAX,:])/pdata[0:TMAX,:])) ** (250 / HOLD) - 1,np.sqrt(250 / HOLD) * np.std(np.mean((pdata))))

#	Average annualized return from holding the Bandit portfolio
print((1+np.mean(reward_avg)) ** (250 / HOLD) - 1 , np.sqrt(250 / HOLD) * np.std(reward_avg))

return_cumulative = np.zeros((TMAX+1,2))
return_cumulative[0,0] = 1
return_cumulative[0,1] = 1
for tt in range(1,TMAX+1):
  return_cumulative[tt,0] = return_cumulative[tt-1,0] * ( 1 + reward_avg[tt-1] )
  rmean = np.mean((pdata[tt+HOLD-1,:]-pdata[tt-1,:])/pdata[tt-1,:])
  return_cumulative[tt,1] = return_cumulative[tt-1,1] * ( 1 + rmean )

plt.plot(pdata_dates[HOLD-1:pdata.shape[0]],return_cumulative[:,0], label='Cumulative return Bandit')
plt.plot(pdata_dates[HOLD-1:pdata.shape[0]],return_cumulative[:,1], label='Cumulative return Average')
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
fig.set_size_inches(12, 3)
plt.xticks(rotation=90)
plt.show()

"""### b) e-greedy algorithm"""

pdata = df_b_returns.to_numpy()
pdata_dates = pd.to_datetime(df_b_returns.index, format='%Y-%m-%d')


#	Bandit problem for stock selection
NK = pdata.shape[1]

EPSILON = 0.4
ALPHA =0.9
NEPISODES = 1000
HOLD = 5
TMAX = pdata.shape[0] - HOLD

#	NEW PARAMETER
UCB_WEIGHT =0.0
seed(1234)
reward_avg = np.zeros((TMAX))
optimal_avg = np.zeros((TMAX))
reward_queue = np.zeros((HOLD,2))

for run in range(NEPISODES):
# Initialize q function and actions record
  qvalue = np.zeros((NK))
  qvalue_up = np.zeros((NK))
  nactions = np.zeros((NK))

  for tt in range(TMAX):
    aa_opt = optimal_action(qvalue_up,EPSILON)
    nactions[aa_opt] += 1

  #	Compute reward as return over holding period
    reward_queue[HOLD-1,0] = (pdata[tt+HOLD,aa_opt]-pdata[tt,aa_opt])/pdata[tt,aa_opt]
    reward_queue[HOLD-1,1] = aa_opt

#	Update Q function using action chosen HOLD days before

    #qvalue = reward_update(int(reward_queue[0,1]), reward_queue[0,0], qvalue, ALPHA)

    qvalue = reward_update(int(reward_queue[0,1]), reward_queue[0,0], qvalue, 1/nactions[aa_opt])
    #print(qvalue)

   #	Upper-confidence adjustment

    qvalue_up = np.zeros((NK))
    for aa in range(NK):
      # If an action has not been visited simply give it the maximum value across actions
      if nactions[aa] == 0:
        qvalue_up[aa] = np.max(qvalue_up) +1.0
      else:
        qvalue_up[aa] = qvalue[aa] + UCB_WEIGHT * np.sqrt(np.log(tt+1)/nactions[aa])

    reward_queue[0:HOLD-1,:] = reward_queue[1:HOLD,:]
    reward_avg[tt] += reward_queue[HOLD-1,0]/NEPISODES
    optimal_avg[tt] += (aa_opt==np.argmax((pdata[tt+HOLD,:]-pdata[tt,:])/pdata[tt,:]))/NEPISODES

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],optimal_avg)

plt.title("Frequency of optimal action epsilon-greedy", fontsize='x-large')
plt.xticks(rotation=90)
fig = plt.gcf()

fig.set_size_inches(12, 3)

plt.show()

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],np.max((pdata[HOLD:pdata.shape[0],:]-pdata[0:TMAX,:])/pdata [0:TMAX,:] , axis=1), label='Max returns')

plt.plot(pdata_dates[HOLD:pdata.shape [0] ],reward_avg, label='Average reward')
plt.xticks(rotation=90)
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
fig.set_size_inches(12, 3)
plt.show()

plt.plot(pdata_dates[HOLD:pdata.shape[0] ],np.mean((pdata[HOLD:pdata.shape[0],:]-pdata[0:TMAX,:])/pdata[0:TMAX,:], axis=1), label='Average returns')
plt.plot(pdata_dates[HOLD:pdata.shape[0]],reward_avg, label='Average reward')
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
plt.xticks(rotation=90)
fig.set_size_inches(12, 3)
plt.show()

#	Average frequency of optimal action
print(np.mean(optimal_avg))

#	Average annualized return from holding the equally-weighted portfolio
print((1 + np.mean((pdata[HOLD:pdata.shape[0] ,:]-pdata[0:TMAX,:])/pdata[0:TMAX,:])) ** (250 / HOLD) - 1,np.sqrt(250 / HOLD) * np.std(np.mean((pdata))))

#	Average annualized return from holding the Bandit portfolio
print((1+np.mean(reward_avg)) ** (250 / HOLD) - 1 , np.sqrt(250 / HOLD) * np.std(reward_avg))

return_cumulative = np.zeros((TMAX+1,2))
return_cumulative[0,0] = 1
return_cumulative[0,1] = 1
for tt in range(1,TMAX+1):
  return_cumulative[tt,0] = return_cumulative[tt-1,0] * ( 1 + reward_avg[tt-1] )
  rmean = np.mean((pdata[tt+HOLD-1,:]-pdata[tt-1,:])/pdata[tt-1,:])
  return_cumulative[tt,1] = return_cumulative[tt-1,1] * ( 1 + rmean )

plt.plot(pdata_dates[HOLD-1:pdata.shape[0]],return_cumulative[:,0], label='Cumulative return Bandit')
plt.plot(pdata_dates[HOLD-1:pdata.shape[0]],return_cumulative[:,1], label='Cumulative return Average')
legend = plt.legend(loc='upper left', shadow=True)
fig = plt.gcf()
fig.set_size_inches(12, 3)
plt.xticks(rotation=90)
plt.show()

