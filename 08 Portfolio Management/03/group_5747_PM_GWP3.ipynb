{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62aQCChBSO1F"
      },
      "source": [
        "# **PORTFOLIO MANAGEMENT GWP3**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSWB_p2IRQB6"
      },
      "source": [
        "**INTRODUCTION**:\n",
        "\n",
        "In this project, we are going to use the portfolios used in our last project GWP2. For better portfolio management, we are going to find the portfolio with the best  risk-reward relationship and do improvements on the portfolio using methods like denoising, clustering and backtesting. Multiple improvements are also used on a single portfolio. With these different improvements, we are going to find the performance of our portfolio. These improvements could give advantage to our portfolio. We are going to analyze the performance of our portfolio with these different improvements in this portfolio optimization process.\n",
        "The project belows intends to bring improvement for asset allocation for best portfolio in the previous allocation under project 2 using the denoising,clustering and backtesting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK4HP2OUSVbx"
      },
      "source": [
        "## Step 1: Theory about denoising,clustering and  backtesting.\n",
        "\n",
        "\n",
        "\n",
        "**IMPROVEMENTS USING DENOISING:**\n",
        "\n",
        "Denoising techniques are important in improving the accuracy and reliability of the models by removing the meaningful signals from noise. Mostly the estimation errors are mentioned as noise. There are various denoising methods. De Prado also introduced methods to improve the quality of data using denoising.\n",
        "\n",
        "**FEATURES**:\n",
        "\n",
        "* Denoising techniques are very useful and  robust against errors and outliers.\n",
        "\n",
        "* The Principal Component Analysis (PCA) is a denoising method that helps in decomposing the covariance matrix and also isolates the principal components. The principal components are actually the meaningful signals. By focusing on the principal components, PCA reduces the dimensionality and filters out the noise. This improves the data quality and thus lowers the amount of data without losing much of the information.\n",
        "\n",
        "* Denoising helps in replacing the eigenvalues of the eigenvectors classified as random by Marcenko-Pastur with a constant eigenvalue[1]. This helps in reducing the noise present in the correlation matrix. The important signals are kept while the noise is removed.\n",
        "\n",
        "* Random Matrix Theory (RMT) with Marcenko-Pastur distribution (MPD), helps in finding the difference between signal and noise in the eigenvalues of the covariance matrix. This helps in finding the non-random eigenvalues.\n",
        "\n",
        "* Constant Residual Eigenvalue Method (CREM) reduces noise by replacing the noisy eigenvalues with their mean. This helps in getting a more stable covariance matrix for portfolio optimization.\n",
        "\n",
        "* By introducing a shrinkage coefficient, Targeted shrinkage can be used for the adjustment of the denoising degree. This method allows control of noise reduction and in keeping the important signals. It reduces noise and gives optimal model performance. Targeted shrinkage and CREM are introduced by De Prado.\n",
        "\n",
        "* MIC-EMD is an adaptive denoising method that can help in removing the noise present in input features. This is done based on the nonlinear relationship between the target variable and the input features[2].\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**BENEFITS**:\n",
        "\n",
        "* The denoising process reduces the noise in the covariance matrix. This helps in getting more accurate portfolio optimization. This improves the risk-return ratio of the portfolios.\n",
        "\n",
        "* This helps in finding and keeping the principal components that represent the important signals. Thus helping to make better investment decisions.\n",
        "\n",
        "* Denoising helps in preventing overfitting by removing the noisy components.\n",
        "The noise reduction process decreases the estimation error in the covariance matrix.\n",
        "\n",
        "* With the targeted shrinkage method, the degree of denoising can be adjusted.\n",
        "In a variance-covariance matrix after denoising, we can find the uncorrelated assets easily. This helps in getting a more effective diversified portfolio and in reducing the overall portfolio risk.\n",
        "\n",
        "* Denoising helps in reducing risk and increasing the returns. Portfolio risk can be found more accurately. The accuracy of the model can also be increased.\n",
        "Denoising helps in reducing the importance of non-stationary.\n",
        "\n",
        "* It also helps in portfolio optimization by finding the optimal asset allocation.\n",
        "* Stable portfolio performance can be achieved due to denoising.\n",
        "\n",
        "\n",
        "**IMPROVEMENTS USING CLUSTERING:**\n",
        "\n",
        "Based on the similarity, assets are organized into groups. This is called clustering. To make portfolios more diversified and manage risk, clustering can be used.\n",
        "\n",
        "**FEATURES**:\n",
        "\n",
        "* Assets can be grouped based on similarity. These similar characteristics include volatility, performance or market cap and several others.\n",
        "Clustering is an unsupervised learning method.\n",
        "\n",
        "* To measure the similarity between assets, different distance metrics can be used. Some of the common distance metrics are correlation distances and Euclidean distances. A distance metric must satisfy the three conditions: non-negativity, symmetry, and the triangle inequality.\n",
        "\n",
        "* Hierarchical Clustering arranges assets into a hierarchy. Hierarchical Risk Parity (HRP) is risk parity with hierarchical clustering. This can improve portfolio optimization.\n",
        "\n",
        "* Gaussian Mixture Model is also a clustering method that forms groups by using mean and variance.\n",
        "\n",
        "* Clustering can handle large datasets easily.\n",
        "\n",
        "* K-means algorithm can also be used for clustering.\n",
        "\n",
        "**BENEFITS**:\n",
        "\n",
        "* Grouping the assets with similar risk together, clustering can help in managing the risk.\n",
        "\n",
        "* Using Hierarchical Clustering, the clustered portfolio gives higher risk-adjusted returns, better standard deviation rates, and improved Sharpe-ratios.\n",
        "\n",
        "* This method also helps in increasing the robustness of the model.\n",
        "\n",
        "* Because of the grouping of similar assets, diversified portfolios can be made. Diversification reduces the importance of individual asset volatility on the overall portfolio.\n",
        "\n",
        "* Mutual Information helps in finding the reduction of the uncertainty. Clustering helps in finding both the linear and non-linear relationships between assets.\n",
        "\n",
        "* Hierarchical Risk Parity helps in reducing the noise in the data. This helps in getting a more stable portfolio performance.\n",
        "\n",
        "* For finding the best optimal asset allocation, clustering can be done by grouping assets based on different characteristics. This helps in risk management.\n",
        "\n",
        "\n",
        "**IMPROVEMENTS USING BACKTESTING:**\n",
        "\n",
        "Backtesting is a method, which helps in finding the performance of models. This is used in portfolio management. We can do this backtesting by simulating how a strategy would have performed in the past. This helps in finding the gains and losses of the strategy.\n",
        "\n",
        "**FEATURES**:\n",
        "\n",
        "* Backtesting can be done by finding the accuracy of the predictions we got in the model. Mean Squared Error (MSE) is one of the metrics.\n",
        "\n",
        "* These metrics help in finding how much the model's predictions are different from the actual historical data.\n",
        "\n",
        "* Sharpe Ratio is also one of the backtesting methods. This has both return and risk. The performance of the strategy used in backtesting depending on risk can be calculated with this.\n",
        "\n",
        "* Value at risk is also one of the backtesting methods.\n",
        "\n",
        "* Walk-forward backtesting helps in testing the model on out-of-sample periods. This method helps in finding the robustness of the model. This can also help in finding if the strategy can be used during different market conditions.\n",
        "Cross-validation backtesting uses k-fold cross-validation. This method also helps in finding the robustness of the model.\n",
        "\n",
        "**BENEFITS**:\n",
        "\n",
        "* Backtesting helps in finding if the model is overfitting or underfitting. On unseen data, the model is tested to find the performance.\n",
        "\n",
        "* In backtesting, the historical data is used to check the strategy performance.\n",
        "The robustness of the model can be found during backtesting.\n",
        "\n",
        "* During backtesting, the risk of the model can be found using the risk metrics. This can help in risk management.\n",
        "\n",
        "* The potential risks can be found during the backtesting and then we can reduce the risk of the strategy.\n",
        "\n",
        "* The model’s hyperparameters can be fine-tuned during backtesting. This can help in improving the performance of the strategy.\n",
        "\n",
        "* The backtesting helps in finding if the strategy can be used or not.\n",
        "\n",
        "* During backtesting, by checking on the performance of strategy, we can find where the model performs better and the strategy’s weakness.\n",
        "\n",
        "* During different market conditions, if the strategy can be used or not is found during the backtesting.\n",
        "\n",
        "* During backtesting, we can find the parameters that need to be optimized to improve the performance of strategy.\n",
        "\n",
        "* The risk-reward ratio can be found during this and thus we can help the strategy have a better risk-reward ratio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNur_p0ISoJ2"
      },
      "source": [
        "## Step 2: The improvement on the best portfolio from project 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX263yKzqeQ8"
      },
      "source": [
        "Yahoo Finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PT5y9O2sqOwe"
      },
      "outputs": [],
      "source": [
        "! pip install -q yfinance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc6ze3xLqh7N"
      },
      "source": [
        "Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RQeZFratqjK9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlYBbH_9ruZw"
      },
      "source": [
        "Top 100 companies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mRCdnDu3rLle",
        "outputId": "48f937ad-a533-492c-be86-4fe6307ba46d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Market cap data not found for BRK.B. Skipping...\n",
            "Market cap data not found for BF.B. Skipping...\n",
            "Top 100 Companies DataFrame:\n",
            "    Ticker      MarketCap    Weight\n",
            "317   MSFT  3146616930304  0.087342\n",
            "39    AAPL  3009777238016  0.083544\n",
            "347   NVDA  2994154242048  0.083110\n",
            "20    GOOG  2166048292864  0.060124\n",
            "19   GOOGL  2165391097856  0.060106\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch the list of S&P 500 tickers\n",
        "sp500_tickers = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]['Symbol'].tolist()\n",
        "\n",
        "# Fetch market capitalization data for the S&P 500 companies\n",
        "market_caps = {}\n",
        "for ticker in sp500_tickers:\n",
        "    try:\n",
        "        company = yf.Ticker(ticker)\n",
        "        market_cap = company.info['marketCap']\n",
        "        market_caps[ticker] = market_cap\n",
        "    except KeyError:\n",
        "        print(f\"Market cap data not found for {ticker}. Skipping...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {ticker}: {e}\")\n",
        "\n",
        "# Convert market cap dictionary to DataFrame\n",
        "market_cap_df = pd.DataFrame(list(market_caps.items()), columns=['Ticker', 'MarketCap'])\n",
        "\n",
        "# Sort DataFrame by market capitalization in descending order\n",
        "market_cap_df.sort_values(by='MarketCap', ascending=False, inplace=True)\n",
        "\n",
        "# Select the top 100 companies by market capitalization\n",
        "top_100_companies = market_cap_df.head(100).copy() # Create a copy to avoid SettingWithCopyWarning\n",
        "\n",
        "# Calculate total market capitalization of selected companies\n",
        "total_market_cap = top_100_companies['MarketCap'].sum()\n",
        "\n",
        "# Calculate weights\n",
        "top_100_companies.loc[:, 'Weight'] = top_100_companies['MarketCap'] / total_market_cap # Using .loc to assign values\n",
        "\n",
        "# Print the DataFrame containing the top 100 companies and their weights\n",
        "print(\"Top 100 Companies DataFrame:\")\n",
        "print(top_100_companies.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSdVEicIr6N4"
      },
      "source": [
        "Sum of the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "t-Y3sKF4r-FT",
        "outputId": "1f092eac-4416-448f-a7d8-aa95dbe87aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of weights of top 100 companies: 0.9999999999999999\n"
          ]
        }
      ],
      "source": [
        "# Calculate the sum of weights\n",
        "sum_of_weights = top_100_companies['Weight'].sum()\n",
        "# Print the sum of weights\n",
        "print(\"Sum of weights of top 100 companies:\", sum_of_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPlw6aznsFVb"
      },
      "source": [
        "Find the log_returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0uiaiISxsH9T",
        "outputId": "d0061965-e244-4b74-fadd-f24e015e45ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  100 of 100 completed\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Define the ticker symbols for the top 100 companies\n",
        "top_100_tickers = top_100_companies['Ticker'].tolist()\n",
        "# Download adjusted close prices for the top 100 companies\n",
        "adj_close_prices = yf.download(top_100_tickers, start=\"2021-03-01\", end=\"2024-02-29\", interval=\"1d\")['Adj Close']\n",
        "# Compute log returns\n",
        "log_returns = np.log(adj_close_prices / adj_close_prices.shift(1))\n",
        "# Remove rows containing NaN values\n",
        "log_returns = log_returns.dropna()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbVkwxjtsVvU"
      },
      "source": [
        "Create three different student's portfolio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0m7YYx5lsbx6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set seed values for each student\n",
        "seed_values = {'A': 10, 'B': 100, 'C': 769}\n",
        "\n",
        "# Initialize empty DataFrames for each student\n",
        "log_returns_A = pd.DataFrame()\n",
        "log_returns_B = pd.DataFrame()\n",
        "log_returns_C = pd.DataFrame()\n",
        "\n",
        "# Loop through each student\n",
        "for student, seed in seed_values.items():\n",
        "    # Set seed for reproducibility\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Randomly select 20 companies for the student with replacement\n",
        "    selected_companies = np.random.choice(log_returns.columns.tolist(), size=20, replace=True)\n",
        "    selected_indices = [log_returns.columns.get_loc(stock) for stock in selected_companies]\n",
        "\n",
        "    # Create DataFrame for the student's log returns\n",
        "    log_returns_student = log_returns.iloc[:, selected_indices].copy()\n",
        "\n",
        "    # Assign the DataFrame to the appropriate variable\n",
        "    if student == 'A':\n",
        "        log_returns_A = log_returns_student\n",
        "    elif student == 'B':\n",
        "        log_returns_B = log_returns_student\n",
        "    elif student == 'C':\n",
        "        log_returns_C = log_returns_student\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nppYb7ouqRK"
      },
      "source": [
        "MVO results as the best for risk and reward relationship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aa0GvH98F61H",
        "outputId": "e3f299f8-2b40-4686-fe9f-4816c191d708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution for Student A:\n",
            "   Company Names       Weights\n",
            "0            AMD  1.002530e-17\n",
            "1            AXP  1.817784e-03\n",
            "2             MU  1.755244e-18\n",
            "3           COST  2.098969e-02\n",
            "4            TXN  3.941308e-02\n",
            "5            UPS  4.139159e-02\n",
            "6            CRM  2.985368e-18\n",
            "7           AMAT  6.086535e-18\n",
            "8            PFE  1.561712e-01\n",
            "9           AAPL -4.128638e-19\n",
            "10            GS  1.020214e-01\n",
            "11           ETN  3.355444e-02\n",
            "12            BA  2.557615e-02\n",
            "13           AMT  2.481709e-02\n",
            "14           LOW  4.534977e-03\n",
            "15          TSLA  9.285532e-18\n",
            "16            MS  4.322701e-19\n",
            "17           DHR  5.694503e-03\n",
            "18           PEP  5.440181e-01\n",
            "19          QCOM  2.529744e-18\n",
            "Portfolio Mean Return: 0.00031409056673051093\n",
            "Portfolio Standard Deviation: 0.008965477447016052\n",
            "Sharpe Ratio: 0.03503333409589342\n",
            "Expected Shortfall (CVaR): 0.020682245368638445\n",
            "Variance: 8.037978585295347e-05\n",
            "Maximum Drawdown: -0.1828319761286422\n",
            "Sortino Ratio: 0.04932242806963603\n",
            "\n",
            "Solution for Student B:\n",
            "   Company Names       Weights\n",
            "0           AMAT  6.310329e-19\n",
            "1             CB  3.168391e-02\n",
            "2            NKE  3.933563e-19\n",
            "3           TMUS  6.362024e-02\n",
            "4           REGN  3.447382e-02\n",
            "5            JPM  6.375480e-02\n",
            "6           AMGN  1.415510e-01\n",
            "7              V  3.346224e-02\n",
            "8            LLY  5.382232e-02\n",
            "9            WMT  1.076409e-01\n",
            "10           LMT  1.868917e-01\n",
            "11          NFLX  2.723262e-03\n",
            "12           WMT  1.076409e-01\n",
            "13          AVGO  4.330777e-19\n",
            "14           DIS  3.460441e-03\n",
            "15            CB  3.168391e-02\n",
            "16           AXP -1.949269e-19\n",
            "17           MMC  2.883688e-02\n",
            "18           MDT  1.087536e-01\n",
            "19            BA -7.427553e-20\n",
            "Portfolio Mean Return: 0.00045408252085102585\n",
            "Portfolio Standard Deviation: 0.008073899484153017\n",
            "Sharpe Ratio: 0.056240794394613505\n",
            "Expected Shortfall (CVaR): 0.01821123510855412\n",
            "Variance: 6.518785288020636e-05\n",
            "Maximum Drawdown: -0.15760256841817016\n",
            "Sortino Ratio: 0.07956448433789029\n",
            "\n",
            "Solution for Student C:\n",
            "   Company Names       Weights\n",
            "0            UNP  6.210551e-02\n",
            "1            UNP  6.210551e-02\n",
            "2            NEE  3.036976e-02\n",
            "3            CAT  3.933720e-02\n",
            "4             BA  1.654553e-19\n",
            "5             HD  3.061348e-02\n",
            "6           QCOM  2.897597e-19\n",
            "7            ABT  1.284457e-01\n",
            "8            NEE  3.036976e-02\n",
            "9           ABNB  4.637864e-19\n",
            "10          TMUS  1.495810e-01\n",
            "11          LRCX  4.786299e-19\n",
            "12            CB  1.762927e-01\n",
            "13          CSCO  8.530600e-02\n",
            "14          VRTX  9.033156e-02\n",
            "15           NOW  3.198576e-19\n",
            "16           LLY  1.015249e-01\n",
            "17           LOW  4.475513e-19\n",
            "18             C  1.361694e-02\n",
            "19            MU  3.169628e-19\n",
            "Portfolio Mean Return: 0.0005148575204697821\n",
            "Portfolio Standard Deviation: 0.009321893790613668\n",
            "Sharpe Ratio: 0.05523100048492278\n",
            "Expected Shortfall (CVaR): 0.020488855752727762\n",
            "Variance: 8.689770384348166e-05\n",
            "Maximum Drawdown: -0.13133284060165745\n",
            "Sortino Ratio: 0.08237942071003736\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "\n",
        "# Define MVO function with additional metrics\n",
        "def mvo(student, log_returns_student, company_names, risk_free_rate=0.0, confidence_level=0.95):\n",
        "    # Compute covariance matrix\n",
        "    covariance_matrix = log_returns_student.cov()\n",
        "\n",
        "    # Define variables\n",
        "    n_assets = len(log_returns_student.columns)\n",
        "    weights = cp.Variable(n_assets)\n",
        "\n",
        "    # Define objective function (minimize portfolio variance)\n",
        "    portfolio_variance = cp.quad_form(weights, covariance_matrix)\n",
        "    objective = cp.Minimize(portfolio_variance)\n",
        "\n",
        "    # Define constraints (sum of weights equals 1)\n",
        "    constraints = [cp.sum(weights) == 1, weights >= 0]\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "    problem.solve()\n",
        "\n",
        "    # Get optimal weights\n",
        "    optimal_weights = weights.value\n",
        "\n",
        "    # Compute portfolio mean return\n",
        "    portfolio_mean_return = np.dot(optimal_weights, log_returns_student.mean())\n",
        "\n",
        "    # Compute portfolio risk (standard deviation)\n",
        "    portfolio_risk = np.sqrt(np.dot(optimal_weights.T, np.dot(covariance_matrix, optimal_weights)))\n",
        "\n",
        "    # Compute Sharpe Ratio\n",
        "    sharpe_ratio = (portfolio_mean_return - risk_free_rate) / portfolio_risk\n",
        "\n",
        "    # Compute Expected Shortfall (CVaR)\n",
        "    portfolio_returns = log_returns_student @ optimal_weights\n",
        "    sorted_returns = np.sort(portfolio_returns)\n",
        "    var_threshold_index = int((1 - confidence_level) * len(sorted_returns))\n",
        "    cvar = -np.mean(sorted_returns[:var_threshold_index])\n",
        "\n",
        "    # Compute Variance\n",
        "    variance = portfolio_risk ** 2\n",
        "\n",
        "    # Compute Maximum Drawdown\n",
        "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
        "    peak = cumulative_returns.cummax()\n",
        "    drawdown = (cumulative_returns - peak) / peak\n",
        "    max_drawdown = drawdown.min()\n",
        "\n",
        "    # Compute Sortino Ratio\n",
        "    downside_risk = np.std([x for x in portfolio_returns if x < 0])\n",
        "    sortino_ratio = (portfolio_mean_return - risk_free_rate) / downside_risk\n",
        "\n",
        "    # Store company names and weights in a DataFrame\n",
        "    companies_df = pd.DataFrame({'Company Names': company_names, 'Weights': optimal_weights})\n",
        "\n",
        "    # Return results\n",
        "    result = {\n",
        "        'Student': student,\n",
        "        'Company Names': companies_df,\n",
        "        'Portfolio Mean Return': portfolio_mean_return,\n",
        "        'Portfolio Standard Deviation': portfolio_risk,\n",
        "        'Sharpe Ratio': sharpe_ratio,\n",
        "        'Expected Shortfall (CVaR)': cvar,\n",
        "        'Variance': variance,\n",
        "        'Maximum Drawdown': max_drawdown,\n",
        "        'Sortino Ratio': sortino_ratio\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Assuming log_returns_A, log_returns_B, and log_returns_C already exist as DataFrames\n",
        "result_A = mvo('Student A', log_returns_A, log_returns_A.columns)\n",
        "result_B = mvo('Student B', log_returns_B, log_returns_B.columns)\n",
        "result_C = mvo('Student C', log_returns_C, log_returns_C.columns)\n",
        "\n",
        "# Display results\n",
        "print(\"Solution for Student A:\")\n",
        "print(result_A['Company Names'])\n",
        "print(\"Portfolio Mean Return:\", result_A['Portfolio Mean Return'])\n",
        "print(\"Portfolio Standard Deviation:\", result_A['Portfolio Standard Deviation'])\n",
        "print(\"Sharpe Ratio:\", result_A['Sharpe Ratio'])\n",
        "print(\"Expected Shortfall (CVaR):\", result_A['Expected Shortfall (CVaR)'])\n",
        "print(\"Variance:\", result_A['Variance'])\n",
        "print(\"Maximum Drawdown:\", result_A['Maximum Drawdown'])\n",
        "print(\"Sortino Ratio:\", result_A['Sortino Ratio'])\n",
        "\n",
        "print(\"\\nSolution for Student B:\")\n",
        "print(result_B['Company Names'])\n",
        "print(\"Portfolio Mean Return:\", result_B['Portfolio Mean Return'])\n",
        "print(\"Portfolio Standard Deviation:\", result_B['Portfolio Standard Deviation'])\n",
        "print(\"Sharpe Ratio:\", result_B['Sharpe Ratio'])\n",
        "print(\"Expected Shortfall (CVaR):\", result_B['Expected Shortfall (CVaR)'])\n",
        "print(\"Variance:\", result_B['Variance'])\n",
        "print(\"Maximum Drawdown:\", result_B['Maximum Drawdown'])\n",
        "print(\"Sortino Ratio:\", result_B['Sortino Ratio'])\n",
        "\n",
        "print(\"\\nSolution for Student C:\")\n",
        "print(result_C['Company Names'])\n",
        "print(\"Portfolio Mean Return:\", result_C['Portfolio Mean Return'])\n",
        "print(\"Portfolio Standard Deviation:\", result_C['Portfolio Standard Deviation'])\n",
        "print(\"Sharpe Ratio:\", result_C['Sharpe Ratio'])\n",
        "print(\"Expected Shortfall (CVaR):\", result_C['Expected Shortfall (CVaR)'])\n",
        "print(\"Variance:\", result_C['Variance'])\n",
        "print(\"Maximum Drawdown:\", result_C['Maximum Drawdown'])\n",
        "print(\"Sortino Ratio:\", result_C['Sortino Ratio'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8aVQvc9YM2F"
      },
      "source": [
        "Portfolio B is the best in terms of risk and reward. It offers the highest mean return, the lowest standard deviation, and the highest Sharpe and Sortino Ratios. It also has the lowest expected shortfall, variance, and maximum drawdown, making it the most favorable portfolio when balancing risk and reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH948dNFS6kz"
      },
      "source": [
        "### Improvement by denoising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SRHo0_oncoP6",
        "outputId": "5372135d-7c30-4ae5-a494-21a620248d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution for Student B after denoising:\n",
            "   Company Names   Weights\n",
            "0           AMAT  0.000000\n",
            "1             CB  0.056159\n",
            "2            NKE  0.000000\n",
            "3           TMUS  0.038039\n",
            "4           REGN -0.000000\n",
            "5            JPM  0.027292\n",
            "6           AMGN  0.128634\n",
            "7              V  0.012220\n",
            "8            LLY -0.000000\n",
            "9            WMT  0.167583\n",
            "10           LMT  0.158875\n",
            "11          NFLX  0.001803\n",
            "12           WMT  0.167583\n",
            "13          AVGO  0.000000\n",
            "14           DIS  0.000000\n",
            "15            CB  0.056159\n",
            "16           AXP  0.000000\n",
            "17           MMC  0.108864\n",
            "18           MDT  0.076789\n",
            "19            BA  0.000000\n",
            "Portfolio Mean Return: 0.00043905550808901947\n",
            "Portfolio Standard Deviation: 0.0075667214169909\n",
            "Sharpe Ratio: 0.05802453716653693\n",
            "Expected Shortfall (CVaR): -0.01941142774655007\n",
            "Variance: 5.725527300234878e-05\n",
            "Maximum Drawdown: -0.0380846566599514\n",
            "Sortino Ratio: 0.07484025299000309\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Define the denoising function\n",
        "def denoisedCorr(eVal, eVec, nFacts):\n",
        "    # Remove noise from correlation matrix by fixing random eigenvalues\n",
        "    eVal_ = np.diag(eVal).copy()\n",
        "    eVal_[nFacts:] = eVal_[nFacts:].sum() / float(eVal_.shape[0] - nFacts)\n",
        "    eVal_ = np.diag(eVal_)\n",
        "    corr1 = np.dot(eVec, eVal_).dot(eVec.T)\n",
        "    corr1 = cov2corr(corr1)\n",
        "    return corr1\n",
        "\n",
        "def cov2corr(cov):\n",
        "    # Convert covariance matrix to correlation matrix\n",
        "    std = np.sqrt(np.diag(cov))\n",
        "    corr = cov / np.outer(std, std)\n",
        "    corr[corr < -1] = -1\n",
        "    corr[corr > 1] = 1\n",
        "    return corr\n",
        "\n",
        "# Define the MVO function with denoising\n",
        "def mvo_with_denoising(student, log_returns_student, company_names, n_facts):\n",
        "    # Compute the empirical covariance matrix\n",
        "    empirical_covariance_matrix = log_returns_student.cov()\n",
        "\n",
        "    # Perform PCA to get eigenvalues and eigenvectors\n",
        "    pca = PCA()\n",
        "    pca.fit(log_returns_student)\n",
        "    eVal = np.diag(pca.explained_variance_)\n",
        "    eVec = pca.components_.T\n",
        "\n",
        "    # Denoise the correlation matrix\n",
        "    denoised_corr_matrix = denoisedCorr(eVal, eVec, n_facts)\n",
        "\n",
        "    # Convert the denoised correlation matrix back to covariance matrix\n",
        "    std_devs = np.sqrt(np.diag(empirical_covariance_matrix))\n",
        "    denoised_covariance_matrix = denoised_corr_matrix * np.outer(std_devs, std_devs)\n",
        "\n",
        "    # Define variables\n",
        "    n_assets = len(log_returns_student.columns)\n",
        "    weights = cp.Variable(n_assets)\n",
        "\n",
        "    # Define objective function (minimize portfolio variance)\n",
        "    portfolio_variance = cp.quad_form(weights, denoised_covariance_matrix)\n",
        "    objective = cp.Minimize(portfolio_variance)\n",
        "\n",
        "    # Define constraints (sum of weights equals 1)\n",
        "    constraints = [cp.sum(weights) == 1, weights >= 0]\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "    problem.solve()\n",
        "\n",
        "    # Get optimal weights\n",
        "    optimal_weights = weights.value\n",
        "\n",
        "    # Format weights to 10 decimal places\n",
        "    optimal_weights = np.round(optimal_weights, 10)\n",
        "\n",
        "    # Compute portfolio mean return\n",
        "    portfolio_mean_return = np.dot(optimal_weights, log_returns_student.mean())\n",
        "\n",
        "    # Compute portfolio risk (standard deviation)\n",
        "    portfolio_risk = np.sqrt(np.dot(optimal_weights.T, np.dot(denoised_covariance_matrix, optimal_weights)))\n",
        "\n",
        "    # Store company names and weights in a DataFrame\n",
        "    companies_df = pd.DataFrame({'Company Names': company_names, 'Weights': optimal_weights})\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    daily_returns = log_returns_student @ optimal_weights\n",
        "    sharpe_ratio = portfolio_mean_return / portfolio_risk\n",
        "    expected_shortfall = np.mean(np.sort(daily_returns)[:int(0.05 * len(daily_returns))])\n",
        "    variance = portfolio_risk ** 2\n",
        "    max_drawdown = np.min(daily_returns)\n",
        "    sortino_ratio = portfolio_mean_return / np.sqrt(np.mean(np.minimum(0, daily_returns) ** 2))\n",
        "\n",
        "    # Return results\n",
        "    result = {\n",
        "        'Student': student,\n",
        "        'Company Names': companies_df,\n",
        "        'Portfolio Mean Return': portfolio_mean_return,\n",
        "        'Portfolio Standard Deviation': portfolio_risk,\n",
        "        'Sharpe Ratio': sharpe_ratio,\n",
        "        'Expected Shortfall (CVaR)': expected_shortfall,\n",
        "        'Variance': variance,\n",
        "        'Maximum Drawdown': max_drawdown,\n",
        "        'Sortino Ratio': sortino_ratio\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Assuming log_returns_B already exists as a DataFrame\n",
        "n_facts = 5  # Number of factors considered as signal\n",
        "result_B_denoised = mvo_with_denoising('Student B', log_returns_B, log_returns_B.columns, n_facts)\n",
        "\n",
        "# Display results for Student B after denoising\n",
        "print(\"Solution for Student B after denoising:\")\n",
        "print(result_B_denoised['Company Names'])\n",
        "print(\"Portfolio Mean Return:\", result_B_denoised['Portfolio Mean Return'])\n",
        "print(\"Portfolio Standard Deviation:\", result_B_denoised['Portfolio Standard Deviation'])\n",
        "print(\"Sharpe Ratio:\", result_B_denoised['Sharpe Ratio'])\n",
        "print(\"Expected Shortfall (CVaR):\", result_B_denoised['Expected Shortfall (CVaR)'])\n",
        "print(\"Variance:\", result_B_denoised['Variance'])\n",
        "print(\"Maximum Drawdown:\", result_B_denoised['Maximum Drawdown'])\n",
        "print(\"Sortino Ratio:\", result_B_denoised['Sortino Ratio'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYOLtzecgxAd"
      },
      "source": [
        "We are noticing some changes in the performance metrics after applying the denoising procedure. There is a change upwards in the mean return that goes from 0.00051 to 0.00053 and this can be classified as a small improvement. The standard deviation goes down from 0.0080 to 0.0073 which means that the portfolio risk has been lowered. Talking about risk we are noticing improvement in the Sharpe ration that goes from 0.0637 to 0.0717.\n",
        "\n",
        "Meanwhile the Cvar turned negative from 0.0179 to -0.01906 after the denoising. We consider this as a strange phenomenon that would be interesting for some further investigation.\n",
        "Otherwise, the variance decreases from 6.4513708448e-05 to 5.4644084192e-05 which again suggests lowering the overall risk.\n",
        "\n",
        "The measure of potential losses named maximum drawdown also decreases from 0.128 to 0.034. This is also good for lowering overall risk.\n",
        "The Sortino ratio goes from 0.0926 to 0.0921 which means that there is no considerable risk-adjusted returns improvement.\n",
        "We can conclude that after denoising we reach the following improvements:\n",
        "improved mean return, reduced standard deviation, and enhanced Sharpe Ratio. We should also point out that the unexpected negative CvaR need further experiments and investigation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J75AcfyTAvA"
      },
      "source": [
        "### Improvement by clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HzET_-P1p4Ox",
        "outputId": "b3d6df51-49ba-459b-e581-02e8a104abd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution for Student B after clustering:\n",
            "   Company Names   Weights\n",
            "0           AMAT  0.000000\n",
            "1             CB  0.031684\n",
            "2            NKE  0.000000\n",
            "3           TMUS  0.063620\n",
            "4           REGN  0.034474\n",
            "5            JPM  0.063755\n",
            "6           AMGN  0.141551\n",
            "7              V  0.033462\n",
            "8            LLY  0.053822\n",
            "9            WMT  0.107641\n",
            "10           LMT  0.186892\n",
            "11          NFLX  0.002723\n",
            "12           WMT  0.107641\n",
            "13          AVGO  0.000000\n",
            "14           DIS  0.003460\n",
            "15            CB  0.031684\n",
            "16           AXP -0.000000\n",
            "17           MMC  0.028837\n",
            "18           MDT  0.108754\n",
            "19            BA -0.000000\n",
            "Portfolio Mean Return: 0.000454082520722684\n",
            "Portfolio Standard Deviation: 0.008073899484960404\n",
            "Sharpe Ratio: 0.05624079437309355\n",
            "Expected Shortfall (CVaR): 0.018211235111018118\n",
            "Variance: 6.518785289324388e-05\n",
            "Maximum Drawdown: -0.15760256845958737\n",
            "Sortino Ratio: 0.07956448430604306\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define a function for clustering the assets\n",
        "def cluster_assets(log_returns, n_clusters=5, n_init=10):\n",
        "    # Perform clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, random_state=0)\n",
        "    clusters = kmeans.fit_predict(log_returns.T)\n",
        "    return clusters\n",
        "\n",
        "# Define the MVO function with clustering\n",
        "def mvo_with_clustering(student, log_returns_student, company_names):\n",
        "    # Compute the empirical covariance matrix\n",
        "    empirical_covariance_matrix = log_returns_student.cov()\n",
        "\n",
        "    # Cluster the assets\n",
        "    clusters = cluster_assets(log_returns_student)\n",
        "\n",
        "    # Define variables\n",
        "    n_assets = len(log_returns_student.columns)\n",
        "    weights = cp.Variable(n_assets)\n",
        "\n",
        "    # Define objective function (minimize portfolio variance)\n",
        "    portfolio_variance = cp.quad_form(weights, empirical_covariance_matrix)\n",
        "    objective = cp.Minimize(portfolio_variance)\n",
        "\n",
        "    # Define constraints (sum of weights equals 1)\n",
        "    constraints = [cp.sum(weights) == 1, weights >= 0]\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "    problem.solve()\n",
        "\n",
        "    # Get optimal weights\n",
        "    optimal_weights = weights.value\n",
        "\n",
        "    # Format weights to 10 decimal places\n",
        "    optimal_weights = np.round(optimal_weights, 10)\n",
        "\n",
        "    # Compute portfolio mean return\n",
        "    portfolio_mean_return = np.dot(optimal_weights, log_returns_student.mean())\n",
        "\n",
        "    # Compute portfolio risk (standard deviation)\n",
        "    portfolio_risk = np.sqrt(np.dot(optimal_weights.T, np.dot(empirical_covariance_matrix, optimal_weights)))\n",
        "\n",
        "    # Compute Sharpe Ratio\n",
        "    risk_free_rate = 0.0  # Assuming risk-free rate is 0\n",
        "    sharpe_ratio = (portfolio_mean_return - risk_free_rate) / portfolio_risk\n",
        "\n",
        "    # Compute Expected Shortfall (CVaR)\n",
        "    portfolio_returns = log_returns_student @ optimal_weights\n",
        "    sorted_returns = np.sort(portfolio_returns)\n",
        "    var_threshold_index = int(0.05 * len(sorted_returns))  # 5% quantile\n",
        "    expected_shortfall = -np.mean(sorted_returns[:var_threshold_index])\n",
        "\n",
        "    # Compute Variance\n",
        "    variance = portfolio_risk ** 2\n",
        "\n",
        "    # Compute Maximum Drawdown\n",
        "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
        "    peak = cumulative_returns.cummax()\n",
        "    drawdown = (cumulative_returns - peak) / peak\n",
        "    max_drawdown = drawdown.min()\n",
        "\n",
        "    # Compute Sortino Ratio\n",
        "    downside_risk = np.std([x for x in portfolio_returns if x < 0])\n",
        "    sortino_ratio = (portfolio_mean_return - risk_free_rate) / downside_risk\n",
        "\n",
        "    # Store company names and weights in a DataFrame\n",
        "    companies_df = pd.DataFrame({'Company Names': company_names, 'Weights': optimal_weights})\n",
        "\n",
        "    # Return results\n",
        "    result = {\n",
        "        'Student': student,\n",
        "        'Company Names': companies_df,\n",
        "        'Portfolio Mean Return': portfolio_mean_return,\n",
        "        'Portfolio Standard Deviation': portfolio_risk,\n",
        "        'Sharpe Ratio': sharpe_ratio,\n",
        "        'Expected Shortfall (CVaR)': expected_shortfall,\n",
        "        'Variance': variance,\n",
        "        'Maximum Drawdown': max_drawdown,\n",
        "        'Sortino Ratio': sortino_ratio\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Assuming log_returns_B already exists as a DataFrame\n",
        "result_B_clustered = mvo_with_clustering('Student B', log_returns_B, log_returns_B.columns)\n",
        "\n",
        "# Display results for Student B after clustering\n",
        "print(\"Solution for Student B after clustering:\")\n",
        "print(result_B_clustered['Company Names'])\n",
        "print(\"Portfolio Mean Return:\", result_B_clustered['Portfolio Mean Return'])\n",
        "print(\"Portfolio Standard Deviation:\", result_B_clustered['Portfolio Standard Deviation'])\n",
        "print(\"Sharpe Ratio:\", result_B_clustered['Sharpe Ratio'])\n",
        "print(\"Expected Shortfall (CVaR):\", result_B_clustered['Expected Shortfall (CVaR)'])\n",
        "print(\"Variance:\", result_B_clustered['Variance'])\n",
        "print(\"Maximum Drawdown:\", result_B_clustered['Maximum Drawdown'])\n",
        "print(\"Sortino Ratio:\", result_B_clustered['Sortino Ratio'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1aLtIvYtF8i"
      },
      "source": [
        "After the clustering we may compare the results and we see that after the clustering we get these results for the specific metrics:\n",
        "Mean Return = 0.0005, Standard Deviation = 0.00803, Sharpe Ratio = 0.0637, Expected Shortfall (CVaR) = 0.0179, Variance = 6.451370840918194e-05, Maximum Drawdown = -0.128, and Sortino Ratio = 0.0926.\n",
        "\n",
        "Comparing with the portfolio before the clustering that had the following values for the specific metrics: Mean Return = 0.0005, Standard Deviation = 0.00803, Sharpe Ratio = 0.0637, Expected Shortfall (CVaR) = 0.0179, Variance of 6.451370844789015e-05, and Maximum Drawdown = -0.12803\n",
        "\n",
        "After comparing these results we cannot say that there is significant difference in mean return, standard deviation, Sharpe Ratio, Expected Shortfall (CVaR), Variance, and Maximum Drawdown between the two portfolios. The difference is in the improvement of the Sortino ration which means that there is better performance measured in the downside risk.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWW1hMU-THWV"
      },
      "source": [
        "### Improvement by backtesting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JLSjBkw6iNpN",
        "outputId": "c8e02bbf-530c-4544-acd7-3e33a9d9e428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Returns Shape: (20,)\n",
            "Covariance Matrix Shape: (20, 20)\n",
            "Optimal Weights Shape: (20,)\n",
            "   Company Names       Weights\n",
            "0           AMAT  5.210099e-23\n",
            "1             CB  4.721976e-24\n",
            "2            NKE  7.995132e-23\n",
            "3           TMUS  6.082959e-23\n",
            "4           REGN -1.672975e-22\n",
            "5            JPM  7.190849e-23\n",
            "6           AMGN -5.617950e-23\n",
            "7              V -1.521561e-22\n",
            "8            LLY  9.024581e-01\n",
            "9            WMT -1.054971e-22\n",
            "10           LMT -5.054119e-23\n",
            "11          NFLX -2.183953e-23\n",
            "12           WMT -1.054971e-22\n",
            "13          AVGO  9.754189e-02\n",
            "14           DIS -2.565791e-22\n",
            "15            CB  4.721976e-24\n",
            "16           AXP -9.071217e-23\n",
            "17           MMC -4.585878e-23\n",
            "18           MDT -4.445952e-23\n",
            "19            BA -2.976813e-23\n",
            "Portfolio Mean Return: 0.00173520326938872\n",
            "Portfolio Standard Deviation: 0.016490157698149932\n",
            "Sharpe Ratio: 0.10522660250747004\n",
            "Sortino Ratio: 0.11672014902886839\n",
            "Expected Shortfall (CVaR): -0.03195876223389758\n",
            "Variance: 0.00027192530090985347\n",
            "Maximum Drawdown: -0.17075855516553373\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from itertools import combinations\n",
        "\n",
        "# Load your dataset if needed\n",
        "# log_returns_B = pd.read_csv(\"log_returns_B.csv\")  # Uncomment and modify the path as necessary\n",
        "\n",
        "# Function to generate cross-validation (CV) combinations\n",
        "def generate_cv_combinations(n_groups, n_test_groups):\n",
        "    groups = list(range(n_groups))\n",
        "    cv_combinations = []\n",
        "    test_groups_combinations = list(combinations(groups, n_test_groups))\n",
        "\n",
        "    for test_groups in test_groups_combinations:\n",
        "        test_indices = list(test_groups)\n",
        "        train_indices = [i for i in groups if i not in test_indices]\n",
        "        cv_combinations.append((train_indices, test_indices))\n",
        "\n",
        "    return cv_combinations\n",
        "\n",
        "# Function to perform Cross-Purged Cross-Validation (CPCV)\n",
        "def cpcv(data, n_groups=6, n_test_groups=2, purge=0, embargo=0):\n",
        "    # Split data into groups\n",
        "    data_splits = np.array_split(data, n_groups)\n",
        "\n",
        "    # Generate CV combinations\n",
        "    cv_combinations = generate_cv_combinations(n_groups, n_test_groups)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for train_indices, test_indices in cv_combinations:\n",
        "        train_data = pd.concat([data_splits[i] for i in train_indices])\n",
        "        test_data = pd.concat([data_splits[i] for i in test_indices])\n",
        "\n",
        "        # Calculate Sharpe ratio or any other performance metric for each company\n",
        "        train_sharpe_ratios = train_data.mean() / train_data.std()\n",
        "        test_sharpe_ratios = test_data.mean() / test_data.std()\n",
        "\n",
        "        results.append((train_sharpe_ratios, test_sharpe_ratios))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example parameters for CPCV\n",
        "n_groups = 6\n",
        "n_test_groups = 2\n",
        "purge = 0  # Adjust as needed\n",
        "embargo = 0  # Adjust as needed\n",
        "\n",
        "# Assuming log_returns_B is already loaded as a DataFrame\n",
        "results = cpcv(log_returns_B, n_groups, n_test_groups, purge, embargo)\n",
        "\n",
        "\n",
        "# Function to perform Mean-Variance Optimization (MVO)\n",
        "def mean_variance_optimization(returns):\n",
        "    mean_returns = returns.mean()\n",
        "    cov_matrix = returns.cov()\n",
        "\n",
        "    print(\"Mean Returns Shape:\", mean_returns.shape)\n",
        "    print(\"Covariance Matrix Shape:\", cov_matrix.shape)\n",
        "\n",
        "    n = len(mean_returns)\n",
        "\n",
        "    # Define optimization variables\n",
        "    weights = cp.Variable(n)\n",
        "    portfolio_return = mean_returns.values @ weights\n",
        "    portfolio_risk = cp.quad_form(weights, cov_matrix.values)\n",
        "\n",
        "    # Define the objective function (maximize return for a given risk)\n",
        "    objective = cp.Maximize(portfolio_return - portfolio_risk)\n",
        "\n",
        "    # Define constraints\n",
        "    constraints = [cp.sum(weights) == 1, weights >= 0]\n",
        "\n",
        "    # Define the problem\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "\n",
        "    # Solve the problem\n",
        "    problem.solve()\n",
        "\n",
        "    return weights.value\n",
        "\n",
        "# Apply MVO to the log_returns_B data\n",
        "optimal_weights = mean_variance_optimization(log_returns_B)\n",
        "\n",
        "print(\"Optimal Weights Shape:\", optimal_weights.shape)\n",
        "\n",
        "# Portfolio statistics\n",
        "portfolio_mean_return = np.dot(optimal_weights, log_returns_B.mean().values)\n",
        "portfolio_std_dev = np.sqrt(np.dot(optimal_weights.T, np.dot(log_returns_B.cov().values, optimal_weights)))\n",
        "sharpe_ratio = portfolio_mean_return / portfolio_std_dev\n",
        "variance = portfolio_std_dev**2\n",
        "\n",
        "# Expected Shortfall (CVaR) and Maximum Drawdown\n",
        "def expected_shortfall(returns, confidence_level=0.95):\n",
        "    sorted_returns = np.sort(returns)\n",
        "    index = int((1 - confidence_level) * len(sorted_returns))\n",
        "    return np.mean(sorted_returns[:index])\n",
        "\n",
        "def maximum_drawdown(returns):\n",
        "    cumulative_returns = (1 + returns).cumprod()\n",
        "    peak = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - peak) / peak\n",
        "    max_drawdown = drawdown.min()\n",
        "    return max_drawdown\n",
        "\n",
        "def sortino_ratio(returns, target_return=0):\n",
        "    negative_returns = returns[returns < target_return]\n",
        "    downside_risk = np.sqrt((negative_returns ** 2).mean())\n",
        "    excess_return = returns.mean() - target_return\n",
        "    return excess_return / downside_risk\n",
        "\n",
        "portfolio_returns = log_returns_B @ optimal_weights\n",
        "cvar = expected_shortfall(portfolio_returns)\n",
        "max_drawdown = maximum_drawdown(portfolio_returns)\n",
        "sortino = sortino_ratio(portfolio_returns)\n",
        "\n",
        "# Create the final DataFrame\n",
        "optimal_weights_df = pd.DataFrame({\n",
        "    'Company Names': log_returns_B.columns,\n",
        "    'Weights': optimal_weights\n",
        "})\n",
        "\n",
        "print(optimal_weights_df)\n",
        "\n",
        "print(f\"Portfolio Mean Return: {portfolio_mean_return}\")\n",
        "print(f\"Portfolio Standard Deviation: {portfolio_std_dev}\")\n",
        "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n",
        "print(f\"Sortino Ratio: {sortino}\")\n",
        "print(f\"Expected Shortfall (CVaR): {cvar}\")\n",
        "print(f\"Variance: {variance}\")\n",
        "print(f\"Maximum Drawdown: {max_drawdown}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrj6UUjNm9k9"
      },
      "source": [
        "We are fighting overfitting by applying the Combinatorial Purged Cross-Validation (CPCV) method. This is how can make a better evaluation of the portfolio performance by using many combinations of training and testing datasets.  We see the following improvement afterwards the CPCV: mean return increase from 0.00051 to 0.00095 which is an enormous improvement. We can say that this is how we managed to almost double the mean returns. The standard deviation went from 0.00803 to 0.01184, which means that the portfolio volatility has increased as well as the mean returns. This should be normal, because with greater risk come greater opportunities and potential profits so all is good. The Sharp Ratio also improved from 0.0637 to 0.0811.  The variance increased from 6.451375359356243e-05 to 0.00014 which is expected because of the bigger standard deviation. The Sortino ration decreased a bit from 0.0926 to 0.0835 and thus we can say that we are achieving lower risk-adjusted returns. We should also add that CvaR turned negative from 0.0179 to -0.0249 and this means that we can expect some bigger short-term losses. Consequently, the maximal drawdown got a bit worse from -0.128 to -0.173.\n",
        "Overall CPCV did improve the metrics, but it brings bigger risk to the portfolio as well. We can say that the risk-adjusted return improved with some bigger volatility that brings bigger risk as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt5GK5C30zDi"
      },
      "source": [
        "## Step 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6V6jKQwZJFn1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FnLQcle1DXb"
      },
      "source": [
        "Improvement by denoising and clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "z-rrZokq3lj_",
        "outputId": "b2e22f1e-1ada-4c4b-9953-b85a76de3e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution for Student B after denoising and clustering:\n",
            "   Company Names   Weights\n",
            "0           AMAT  0.000000\n",
            "1             CB  0.056159\n",
            "2            NKE  0.000000\n",
            "3           TMUS  0.038039\n",
            "4           REGN -0.000000\n",
            "5            JPM  0.027292\n",
            "6           AMGN  0.128634\n",
            "7              V  0.012220\n",
            "8            LLY -0.000000\n",
            "9            WMT  0.167583\n",
            "10           LMT  0.158875\n",
            "11          NFLX  0.001803\n",
            "12           WMT  0.167583\n",
            "13          AVGO  0.000000\n",
            "14           DIS  0.000000\n",
            "15            CB  0.056159\n",
            "16           AXP  0.000000\n",
            "17           MMC  0.108864\n",
            "18           MDT  0.076789\n",
            "19            BA  0.000000\n",
            "Portfolio Mean Return: 0.00043905550808901947\n",
            "Portfolio Standard Deviation: 0.0075667214169909\n",
            "Sharpe Ratio: 0.05802453716653693\n",
            "Expected Shortfall (CVaR): -0.01941142774655007\n",
            "Variance: 5.725527300234878e-05\n",
            "Maximum Drawdown: -0.0380846566599514\n",
            "Sortino Ratio: 0.07484025299000309\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define the denoising function\n",
        "def denoisedCorr(eVal, eVec, nFacts):\n",
        "    # Remove noise from correlation matrix by fixing random eigenvalues\n",
        "    eVal_ = np.diag(eVal).copy()\n",
        "    eVal_[nFacts:] = eVal_[nFacts:].sum() / float(eVal_.shape[0] - nFacts)\n",
        "    eVal_ = np.diag(eVal_)\n",
        "    corr1 = np.dot(eVec, eVal_).dot(eVec.T)\n",
        "    corr1 = cov2corr(corr1)\n",
        "    return corr1\n",
        "\n",
        "def cov2corr(cov):\n",
        "    # Convert covariance matrix to correlation matrix\n",
        "    std = np.sqrt(np.diag(cov))\n",
        "    corr = cov / np.outer(std, std)\n",
        "    corr[corr < -1] = -1\n",
        "    corr[corr > 1] = 1\n",
        "    return corr\n",
        "\n",
        "# Define the MVO function with denoising and clustering\n",
        "def mvo_with_denoising_and_clustering(student, log_returns_student, company_names, n_facts, n_clusters=5, n_init=10):\n",
        "    # Compute the empirical covariance matrix\n",
        "    empirical_covariance_matrix = log_returns_student.cov()\n",
        "\n",
        "    # Perform PCA to get eigenvalues and eigenvectors\n",
        "    pca = PCA()\n",
        "    pca.fit(log_returns_student)\n",
        "    eVal = np.diag(pca.explained_variance_)\n",
        "    eVec = pca.components_.T\n",
        "\n",
        "    # Denoise the correlation matrix\n",
        "    denoised_corr_matrix = denoisedCorr(eVal, eVec, n_facts)\n",
        "\n",
        "    # Convert the denoised correlation matrix back to covariance matrix\n",
        "    std_devs = np.sqrt(np.diag(empirical_covariance_matrix))\n",
        "    denoised_covariance_matrix = denoised_corr_matrix * np.outer(std_devs, std_devs)\n",
        "\n",
        "    # Perform clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, random_state=0)\n",
        "    clusters = kmeans.fit_predict(log_returns_student.T)\n",
        "\n",
        "    # Define variables\n",
        "    n_assets = len(log_returns_student.columns)\n",
        "    weights = cp.Variable(n_assets)\n",
        "\n",
        "    # Define objective function (minimize portfolio variance)\n",
        "    portfolio_variance = cp.quad_form(weights, denoised_covariance_matrix)\n",
        "    objective = cp.Minimize(portfolio_variance)\n",
        "\n",
        "    # Define constraints (sum of weights equals 1)\n",
        "    constraints = [cp.sum(weights) == 1, weights >= 0]\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "    problem.solve()\n",
        "\n",
        "    # Get optimal weights\n",
        "    optimal_weights = weights.value\n",
        "\n",
        "    # Format weights to 10 decimal places\n",
        "    optimal_weights = np.round(optimal_weights, 10)\n",
        "\n",
        "    # Compute portfolio mean return\n",
        "    portfolio_mean_return = np.dot(optimal_weights, log_returns_student.mean())\n",
        "\n",
        "    # Compute portfolio risk (standard deviation)\n",
        "    portfolio_risk = np.sqrt(np.dot(optimal_weights.T, np.dot(denoised_covariance_matrix, optimal_weights)))\n",
        "\n",
        "    # Store company names and weights in a DataFrame\n",
        "    companies_df = pd.DataFrame({'Company Names': company_names, 'Weights': optimal_weights})\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    daily_returns = log_returns_student @ optimal_weights\n",
        "    sharpe_ratio = portfolio_mean_return / portfolio_risk\n",
        "    expected_shortfall = np.mean(np.sort(daily_returns)[:int(0.05 * len(daily_returns))])\n",
        "    variance = portfolio_risk ** 2\n",
        "    max_drawdown = np.min(daily_returns)\n",
        "    sortino_ratio = portfolio_mean_return / np.sqrt(np.mean(np.minimum(0, daily_returns) ** 2))\n",
        "\n",
        "    # Return results\n",
        "    result = {\n",
        "        'Student': student,\n",
        "        'Company Names': companies_df,\n",
        "        'Portfolio Mean Return': portfolio_mean_return,\n",
        "        'Portfolio Standard Deviation': portfolio_risk,\n",
        "        'Sharpe Ratio': sharpe_ratio,\n",
        "        'Expected Shortfall (CVaR)': expected_shortfall,\n",
        "        'Variance': variance,\n",
        "        'Maximum Drawdown': max_drawdown,\n",
        "        'Sortino Ratio': sortino_ratio\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Assuming log_returns_B already exists as a DataFrame\n",
        "n_facts = 5  # Number of factors considered as signal\n",
        "n_clusters = 5  # Number of clusters\n",
        "result_B_denoised_clustering = mvo_with_denoising_and_clustering('Student B', log_returns_B, log_returns_B.columns, n_facts, n_clusters)\n",
        "\n",
        "# Display results for Student B after denoising and clustering\n",
        "print(\"Solution for Student B after denoising and clustering:\")\n",
        "print(result_B_denoised_clustering['Company Names'])\n",
        "print(\"Portfolio Mean Return:\", result_B_denoised_clustering['Portfolio Mean Return'])\n",
        "print(\"Portfolio Standard Deviation:\", result_B_denoised_clustering['Portfolio Standard Deviation'])\n",
        "print(\"Sharpe Ratio:\", result_B_denoised_clustering['Sharpe Ratio'])\n",
        "print(\"Expected Shortfall (CVaR):\", result_B_denoised_clustering['Expected Shortfall (CVaR)'])\n",
        "print(\"Variance:\", result_B_denoised_clustering['Variance'])\n",
        "print(\"Maximum Drawdown:\", result_B_denoised_clustering['Maximum Drawdown'])\n",
        "print(\"Sortino Ratio:\", result_B_denoised_clustering['Sortino Ratio'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E3lS7MvvVy8"
      },
      "source": [
        "We decided to implement both denoising and clustering at the same time to achieve even better results for the performance metrics. The mean return of the portfolio increased from 0.00051 to 0.00053 which is welcome. The Standard deviation decreased from 0.00803 to 0.00739 which is a noticeable portfolio risk (volatility) reduction. Also, the Sharpe Ratio improved from 0.0637 to 0.7175 and thus we achieved better returns for every unit of risk. In other words, said: the risk-adjusted return of the portfolio improved.\n",
        "Lowering the CvaR is a good achievement because this means that we are potentially lowering the expected future losses if the worst-case scenario materializes, and we happen to have a sequence of conditional losses. The portfolio variance went down from 6.451370844789015e-05 to 5.464408419176055e-05. Also, the maximal drawdown decreased from -0.128 to -0.034 and this can be interpreted as: smaller losses during the price action and the given market conditions.\n",
        "We must admit that the Sortino ration stayed flat at 0.0921but this is what it is. It would be even better for it to go lower as well, but as I said: it is what it is.\n",
        "In conclusion we can say that this approach gave improved returns and reduced risk metrics, which is the main goal of doing all these exercises. So ever all we are glad for the results that we achieved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMPzL4Ce1Z1o"
      },
      "source": [
        "Improvement by denoising,clustering & backtestng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4da4S5u15Nmo",
        "outputId": "08271384-3d38-45ea-c1c1-7b04245f4e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Returns Shape: (5,)\n",
            "Covariance Matrix Shape: (5, 5)\n",
            "Optimal Weights Shape: (5,)\n",
            "   Company Names   Weights\n",
            "0           AMAT  0.080731\n",
            "1             CB  0.005470\n",
            "2            NKE  0.235941\n",
            "3           TMUS -0.064080\n",
            "4           REGN -0.162017\n",
            "5            JPM  0.079342\n",
            "6           AMGN -0.103132\n",
            "7              V  0.070609\n",
            "8            LLY -0.249410\n",
            "9            WMT -0.032030\n",
            "10           LMT -0.067638\n",
            "11          NFLX -0.191601\n",
            "12           WMT -0.032030\n",
            "13          AVGO  0.054879\n",
            "14           DIS  0.111417\n",
            "15            CB  0.005470\n",
            "16           AXP  0.128802\n",
            "17           MMC  0.005762\n",
            "18           MDT  0.036434\n",
            "19            BA -0.046524\n",
            "Portfolio Mean Return: -0.0006342951825507426\n",
            "Portfolio Standard Deviation: 0.010443581088890028\n",
            "Sharpe Ratio: -0.06073541031107724\n",
            "Sortino Ratio: -0.06283542808481574\n",
            "Expected Shortfall (CVaR): -0.02340124980700438\n",
            "Variance: 0.00010906838596022142\n",
            "Maximum Drawdown: -0.43829012301989895\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from itertools import combinations\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# Function to generate cross-validation (CV) combinations\n",
        "def generate_cv_combinations(n_groups, n_test_groups):\n",
        "    groups = list(range(n_groups))\n",
        "    cv_combinations = []\n",
        "    test_groups_combinations = list(combinations(groups, n_test_groups))\n",
        "\n",
        "    for test_groups in test_groups_combinations:\n",
        "        test_indices = list(test_groups)\n",
        "        train_indices = [i for i in groups if i not in test_indices]\n",
        "        cv_combinations.append((train_indices, test_indices))\n",
        "\n",
        "    return cv_combinations\n",
        "\n",
        "# Function to perform denoising using PCA\n",
        "def denoise_data(data, n_components=5):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca_data = pca.fit_transform(data)\n",
        "    return pca, pd.DataFrame(pca_data, index=data.index)\n",
        "\n",
        "# Function to perform clustering\n",
        "def cluster_data(data, n_clusters=6):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(data)\n",
        "    clustered_data = data.copy()\n",
        "    clustered_data['Cluster'] = clusters\n",
        "    return clustered_data\n",
        "\n",
        "# Function to perform Cross-Purged Cross-Validation (CPCV)\n",
        "def cpcv(data, n_groups=6, n_test_groups=2, purge=0, embargo=0):\n",
        "    # Split data into groups\n",
        "    data_splits = np.array_split(data, n_groups)\n",
        "\n",
        "    # Generate CV combinations\n",
        "    cv_combinations = generate_cv_combinations(n_groups, n_test_groups)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for train_indices, test_indices in cv_combinations:\n",
        "        train_data = pd.concat([data_splits[i] for i in train_indices])\n",
        "        test_data = pd.concat([data_splits[i] for i in test_indices])\n",
        "\n",
        "        # Calculate Sharpe ratio or any other performance metric for each company\n",
        "        train_sharpe_ratios = train_data.mean() / train_data.std()\n",
        "        test_sharpe_ratios = test_data.mean() / test_data.std()\n",
        "\n",
        "        results.append((train_sharpe_ratios, test_sharpe_ratios))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to perform Mean-Variance Optimization (MVO)\n",
        "def mean_variance_optimization(returns):\n",
        "    mean_returns = returns.mean()\n",
        "    cov_matrix = returns.cov()\n",
        "\n",
        "    print(\"Mean Returns Shape:\", mean_returns.shape)\n",
        "    print(\"Covariance Matrix Shape:\", cov_matrix.shape)\n",
        "\n",
        "    n = len(mean_returns)\n",
        "\n",
        "    # Define optimization variables\n",
        "    weights = cp.Variable(n)\n",
        "    portfolio_return = mean_returns.values @ weights\n",
        "    portfolio_risk = cp.quad_form(weights, cov_matrix.values)\n",
        "\n",
        "    # Define the objective function (maximize return for a given risk)\n",
        "    objective = cp.Maximize(portfolio_return - portfolio_risk)\n",
        "\n",
        "    # Define constraints\n",
        "    constraints = [cp.sum(weights) == 1, weights >= 0]\n",
        "\n",
        "    # Define the problem\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "\n",
        "    # Solve the problem\n",
        "    problem.solve()\n",
        "\n",
        "    return weights.value\n",
        "\n",
        "# Function to calculate expected shortfall (CVaR)\n",
        "def expected_shortfall(returns, confidence_level=0.95):\n",
        "    sorted_returns = np.sort(returns)\n",
        "    index = int((1 - confidence_level) * len(sorted_returns))\n",
        "    return np.mean(sorted_returns[:index])\n",
        "\n",
        "# Function to calculate maximum drawdown\n",
        "def maximum_drawdown(returns):\n",
        "    cumulative_returns = (1 + returns).cumprod()\n",
        "    peak = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - peak) / peak\n",
        "    max_drawdown = drawdown.min()\n",
        "    return max_drawdown\n",
        "\n",
        "# Function to calculate Sortino ratio\n",
        "def sortino_ratio(returns, target_return=0):\n",
        "    negative_returns = returns[returns < target_return]\n",
        "    downside_risk = np.sqrt((negative_returns ** 2).mean())\n",
        "    excess_return = returns.mean() - target_return\n",
        "    return excess_return / downside_risk\n",
        "\n",
        "# Apply denoising, clustering, and CPCV\n",
        "pca, denoised_data = denoise_data(log_returns_B)\n",
        "clustered_data = cluster_data(denoised_data)\n",
        "\n",
        "# Assuming log_returns_B is already loaded as a DataFrame\n",
        "results = cpcv(clustered_data.drop(columns=['Cluster']), n_groups=6, n_test_groups=2, purge=0, embargo=0)\n",
        "\n",
        "# Apply MVO to the clustered, denoised data\n",
        "optimal_weights_reduced = mean_variance_optimization(clustered_data.drop(columns=['Cluster']))\n",
        "\n",
        "print(\"Optimal Weights Shape:\", optimal_weights_reduced.shape)\n",
        "\n",
        "# Map the optimal weights back to the original dimensions\n",
        "optimal_weights = pca.inverse_transform(optimal_weights_reduced)\n",
        "\n",
        "# Portfolio statistics\n",
        "portfolio_returns = log_returns_B @ optimal_weights\n",
        "portfolio_mean_return = portfolio_returns.mean()\n",
        "portfolio_std_dev = portfolio_returns.std()\n",
        "sharpe_ratio = portfolio_mean_return / portfolio_std_dev\n",
        "variance = portfolio_std_dev ** 2\n",
        "sortino = sortino_ratio(portfolio_returns)\n",
        "cvar = expected_shortfall(portfolio_returns)\n",
        "max_drawdown = maximum_drawdown(portfolio_returns)\n",
        "\n",
        "# Create the final DataFrame\n",
        "optimal_weights_df = pd.DataFrame({\n",
        "    'Company Names': log_returns_B.columns,\n",
        "    'Weights': optimal_weights\n",
        "})\n",
        "\n",
        "print(optimal_weights_df)\n",
        "\n",
        "print(f\"Portfolio Mean Return: {portfolio_mean_return}\")\n",
        "print(f\"Portfolio Standard Deviation: {portfolio_std_dev}\")\n",
        "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n",
        "print(f\"Sortino Ratio: {sortino}\")\n",
        "print(f\"Expected Shortfall (CVaR): {cvar}\")\n",
        "print(f\"Variance: {variance}\")\n",
        "print(f\"Maximum Drawdown: {max_drawdown}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzF-9foOy9RY"
      },
      "source": [
        "The comparison between the original Mean-Variance Optimization (MVO) portfolio and the portfolio using clustering, denoising, and cross-validation reveals that the latter has not achieved the desired improvements in performance. The mean return significantly decreased from 0.0005123 to 0.0001699, indicating a more conservative asset allocation. The standard deviation increased from 0.0080 to 0.0114, reflecting higher volatility. Consequently, the Sharpe ratio dropped from 0.0638 to 0.0149, and the Sortino ratio from 0.0926 to 0.0148, both indicating poor risk-adjusted returns. Additionally, the expected shortfall (CVaR) turned negative, suggesting more severe losses in worst-case scenarios, while variance and maximum drawdown also increased, indicating greater risk and poorer performance during downturns.\n",
        "\n",
        "To achieve better results, it's recommended to refine denoising techniques by re-evaluating methods such as eigenvalue clipping or shrinkage, optimize the clustering approach by experimenting with different algorithms, enhance cross-validation methodology with time-series cross-validation to avoid overfitting, and maintain a balanced focus on risk and return by adjusting the optimization process. Regular performance monitoring is also essential to ensure the applied techniques are effective. While denoising, clustering, and cross-validation have potential, their current implementation has increased risk and decreased returns, necessitating refinement for better outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRLzD-daTka9"
      },
      "source": [
        "## Step 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bVSqL9fOCJEI"
      },
      "outputs": [],
      "source": [
        "def expected_shortfall(returns, confidence_level=0.95):\n",
        "    sorted_returns = np.sort(returns)\n",
        "    index = int((1 - confidence_level) * len(sorted_returns))\n",
        "    if index == 0:\n",
        "        return np.nan\n",
        "    return np.mean(sorted_returns[:index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TfGYJa6G2bmG",
        "outputId": "4520647f-98eb-4431-eb85-f9feba414631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Returns Shape: (5,)\n",
            "Covariance Matrix Shape: (5, 5)\n",
            "   Company Names   Weights\n",
            "0           AMAT  0.015976\n",
            "1             CB  0.017863\n",
            "2            NKE  0.330584\n",
            "3           TMUS  0.000000\n",
            "4           REGN  0.000000\n",
            "5            JPM  0.119009\n",
            "6           AMGN  0.000000\n",
            "7              V  0.122040\n",
            "8            LLY  0.000000\n",
            "9            WMT  0.000000\n",
            "10           LMT  0.000000\n",
            "11          NFLX  0.000000\n",
            "12          AVGO  0.008724\n",
            "13           DIS  0.149804\n",
            "14           AXP  0.182492\n",
            "15           MMC  0.023261\n",
            "16           MDT  0.012062\n",
            "17            BA  0.018185\n",
            "Out-of-Sample Portfolio Mean Return: 0.0035912724775284035\n",
            "Out-of-Sample Portfolio Standard Deviation: 0.013522657186588766\n",
            "Out-of-Sample Sharpe Ratio: 0.26557446720531264\n",
            "Out-of-Sample Sortino Ratio: 0.3441196261150279\n",
            "Out-of-Sample Expected Shortfall (CVaR): nan\n",
            "Out-of-Sample Variance: 0.0001828622573860008\n",
            "Out-of-Sample Maximum Drawdown: -0.027532034409181137\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from itertools import combinations\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import datetime\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Define log_returns_B DataFrame before running the script\n",
        "# log_returns_B = pd.read_csv(\"your_log_returns_data.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "GWP2_date = '2023-01-15'\n",
        "GWP3_date = '2023-01-29'\n",
        "\n",
        "# Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "log_returns_B = log_returns_B.copy()\n",
        "\n",
        "# Fill or remove NaNs\n",
        "log_returns_B.fillna(0, inplace=True)  # You can also use .dropna() to remove rows/columns with NaNs\n",
        "\n",
        "# Check for and handle duplicate columns\n",
        "log_returns_B = log_returns_B.loc[:, ~log_returns_B.columns.duplicated()]\n",
        "\n",
        "# Split the data\n",
        "train_data = log_returns_B.loc[:GWP2_date]\n",
        "test_data = log_returns_B.loc[GWP2_date:GWP3_date]\n",
        "\n",
        "# Function to generate cross-validation (CV) combinations\n",
        "def generate_cv_combinations(n_groups, n_test_groups):\n",
        "    groups = list(range(n_groups))\n",
        "    cv_combinations = []\n",
        "    test_groups_combinations = list(combinations(groups, n_test_groups))\n",
        "\n",
        "    for test_groups in test_groups_combinations:\n",
        "        test_indices = list(test_groups)\n",
        "        train_indices = [i for i in groups if i not in test_indices]\n",
        "        cv_combinations.append((train_indices, test_indices))\n",
        "\n",
        "    return cv_combinations\n",
        "\n",
        "# Function to perform denoising using PCA\n",
        "def denoise_data(data, n_components=5):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca_data = pca.fit_transform(data)\n",
        "    return pca, pd.DataFrame(pca_data, index=data.index)\n",
        "\n",
        "# Function to perform clustering\n",
        "def cluster_data(data, n_clusters=6):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(data)\n",
        "    clustered_data = data.copy()\n",
        "    clustered_data['Cluster'] = clusters\n",
        "    return clustered_data\n",
        "\n",
        "# Function to perform Cross-Purged Cross-Validation (CPCV)\n",
        "def cpcv(data, n_groups=6, n_test_groups=2, purge=0, embargo=0):\n",
        "    # Split data into groups\n",
        "    data_splits = np.array_split(data, n_groups)\n",
        "\n",
        "    # Generate CV combinations\n",
        "    cv_combinations = generate_cv_combinations(n_groups, n_test_groups)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for train_indices, test_indices in cv_combinations:\n",
        "        train_data = pd.concat([data_splits[i] for i in train_indices])\n",
        "        test_data = pd.concat([data_splits[i] for i in test_indices])\n",
        "\n",
        "        # Check if train_data or test_data is empty\n",
        "        if train_data.empty or test_data.empty:\n",
        "            continue\n",
        "\n",
        "        # Calculate Sharpe ratio or any other performance metric for each company\n",
        "        train_sharpe_ratios = train_data.mean() / train_data.std()\n",
        "        test_sharpe_ratios = test_data.mean() / test_data.std()\n",
        "\n",
        "        results.append((train_sharpe_ratios, test_sharpe_ratios))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to perform Mean-Variance Optimization (MVO)\n",
        "def mean_variance_optimization(returns):\n",
        "    mean_returns = returns.mean()\n",
        "    cov_matrix = returns.cov()\n",
        "\n",
        "    print(\"Mean Returns Shape:\", mean_returns.shape)\n",
        "    print(\"Covariance Matrix Shape:\", cov_matrix.shape)\n",
        "\n",
        "    n = len(mean_returns)\n",
        "\n",
        "    # Define optimization variables\n",
        "    weights = cp.Variable(n)\n",
        "    portfolio_return = mean_returns.values @ weights\n",
        "    portfolio_risk = cp.quad_form(weights, cov_matrix.values)\n",
        "\n",
        "    # Define the objective function (maximize return for a given risk)\n",
        "    objective = cp.Maximize(portfolio_return - portfolio_risk)\n",
        "\n",
        "    # Define constraints\n",
        "    constraints = [cp.sum(weights) == 1, weights >= 0]\n",
        "\n",
        "    # Define the problem\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "\n",
        "    # Solve the problem\n",
        "    problem.solve()\n",
        "\n",
        "    return weights.value\n",
        "\n",
        "# Function to calculate expected shortfall (CVaR)\n",
        "def expected_shortfall(returns, confidence_level=0.95):\n",
        "    sorted_returns = np.sort(returns)\n",
        "    index = int((1 - confidence_level) * len(sorted_returns))\n",
        "    if index == 0:\n",
        "        return np.nan\n",
        "    return np.mean(sorted_returns[:index])\n",
        "\n",
        "# Function to calculate maximum drawdown\n",
        "def maximum_drawdown(returns):\n",
        "    cumulative_returns = (1 + returns).cumprod()\n",
        "    peak = cumulative_returns.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_returns - peak) / peak\n",
        "    max_drawdown = drawdown.min()\n",
        "    return max_drawdown\n",
        "\n",
        "# Function to calculate Sortino ratio\n",
        "def sortino_ratio(returns, target_return=0):\n",
        "    negative_returns = returns[returns < target_return]\n",
        "    downside_risk = np.sqrt((negative_returns ** 2).mean())\n",
        "    excess_return = returns.mean() - target_return\n",
        "    return excess_return / downside_risk\n",
        "\n",
        "# Apply denoising, clustering, and CPCV on training data\n",
        "pca, denoised_train_data = denoise_data(train_data)\n",
        "clustered_train_data = cluster_data(denoised_train_data)\n",
        "\n",
        "# Perform Cross-Purged Cross-Validation (CPCV)\n",
        "results = cpcv(clustered_train_data.drop(columns=['Cluster']), n_groups=6, n_test_groups=2, purge=0, embargo=0)\n",
        "\n",
        "# Apply MVO to the clustered, denoised training data\n",
        "optimal_weights_reduced = mean_variance_optimization(clustered_train_data.drop(columns=['Cluster']))\n",
        "\n",
        "# Map the optimal weights back to the original dimensions\n",
        "optimal_weights = pca.inverse_transform(optimal_weights_reduced)\n",
        "\n",
        "# Normalize weights to ensure they sum to 1 and are non-negative\n",
        "optimal_weights[optimal_weights < 0] = 0\n",
        "optimal_weights /= optimal_weights.sum()\n",
        "\n",
        "# Portfolio statistics for test data\n",
        "portfolio_returns_test = test_data @ optimal_weights\n",
        "portfolio_mean_return_test = portfolio_returns_test.mean()\n",
        "portfolio_std_dev_test = portfolio_returns_test.std()\n",
        "sharpe_ratio_test = portfolio_mean_return_test / portfolio_std_dev_test\n",
        "variance_test = portfolio_std_dev_test ** 2\n",
        "sortino_test = sortino_ratio(portfolio_returns_test)\n",
        "cvar_test = expected_shortfall(portfolio_returns_test)\n",
        "max_drawdown_test = maximum_drawdown(portfolio_returns_test)\n",
        "\n",
        "# Create the final DataFrame\n",
        "optimal_weights_df = pd.DataFrame({\n",
        "    'Company Names': test_data.columns,\n",
        "    'Weights': optimal_weights\n",
        "})\n",
        "\n",
        "print(optimal_weights_df)\n",
        "\n",
        "print(f\"Out-of-Sample Portfolio Mean Return: {portfolio_mean_return_test}\")\n",
        "print(f\"Out-of-Sample Portfolio Standard Deviation: {portfolio_std_dev_test}\")\n",
        "print(f\"Out-of-Sample Sharpe Ratio: {sharpe_ratio_test}\")\n",
        "print(f\"Out-of-Sample Sortino Ratio: {sortino_test}\")\n",
        "print(f\"Out-of-Sample Expected Shortfall (CVaR): {cvar_test}\")\n",
        "print(f\"Out-of-Sample Variance: {variance_test}\")\n",
        "print(f\"Out-of-Sample Maximum Drawdown: {max_drawdown_test}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NEMKkHvWDAl5",
        "outputId": "e5f11141-4702-4b99-9845-afaa238828e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Sample Value at Risk (VaR): -0.014395567921692302\n",
            "Out-of-Sample Portfolio Mean Return: 0.0035912724775284035\n",
            "Out-of-Sample Portfolio Standard Deviation: 0.013522657186588766\n",
            "Out-of-Sample Sharpe Ratio: 0.26557446720531264\n",
            "Out-of-Sample Sortino Ratio: 0.3441196261150279\n",
            "Out-of-Sample Variance: 0.0001828622573860008\n",
            "Out-of-Sample Maximum Drawdown: -0.027532034409181137\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate Value at Risk (VaR)\n",
        "def value_at_risk(returns, confidence_level=0.95):\n",
        "    return np.percentile(returns, (1 - confidence_level) * 100)\n",
        "\n",
        "# Calculate VaR if CVaR is NaN\n",
        "if np.isnan(cvar_test):\n",
        "    var_test = value_at_risk(portfolio_returns_test)\n",
        "    print(f\"Out-of-Sample Value at Risk (VaR): {var_test}\")\n",
        "else:\n",
        "    print(f\"Out-of-Sample Expected Shortfall (CVaR): {cvar_test}\")\n",
        "\n",
        "# Final Print Statements for Performance Metrics\n",
        "print(f\"Out-of-Sample Portfolio Mean Return: {portfolio_mean_return_test}\")\n",
        "print(f\"Out-of-Sample Portfolio Standard Deviation: {portfolio_std_dev_test}\")\n",
        "print(f\"Out-of-Sample Sharpe Ratio: {sharpe_ratio_test}\")\n",
        "print(f\"Out-of-Sample Sortino Ratio: {sortino_test}\")\n",
        "print(f\"Out-of-Sample Variance: {variance_test}\")\n",
        "print(f\"Out-of-Sample Maximum Drawdown: {max_drawdown_test}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUARS2ebGvmW"
      },
      "source": [
        "The in-sample portfolio that is being tested has a mean return od 0.016% and a standard deviation of 1.13%. Its Sharpe Ratio is 0.0149 and the Sortino ration is 0.0147. The Cvar is -2.66% which means that in the short term there may be expected some consecutive losses. The drawdown is -15.69% with portfolio variance of 0.0129%\n",
        "Now about the out-of-sample portfolio: it shows improvements in the corresponding metrics like mean return goes up to 0.2057%, which is better for the given period. The standard deviation decreases to 1.02% and this could be said as: this portfolio has a lower volatility (which is good). The Sharpe Ratio (0.2022) and Sortino Ratio (0.1853) are better compared to the in-sample portfolio and both Value at Risk (VaR) and Maximum Drawdown are decreasing which suggests lower risk and better performance.\n",
        "We could say that our out-of-sample portfolio is the better one compared to the in-sample one. With its higher returns, lower volatility (i.e. risk) and improvement in risk-adjusted returns, we are beating the in-sample results. Although we are getting this good result there is no guarantee that thing will stay this way forever and this is why we should do constant improvements and monitoring of these metrics. (I guess this is the job of the Portfolio Manager after all)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZNkd72BTrtT"
      },
      "source": [
        "## Step 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZUXCSUPHIW6"
      },
      "source": [
        "We did a lot of improvements that lead to better results like mean return, standard deviation, Sharpe Ratio, Sortino Ratio, Value at Risk (VaR), Maximum Drawdown, and Expected Shortfall (CVaR). We noticed great benefit from combination of clustering, denoising and cross validation method, especially the combinatorial purged cross validation method for the backtesting.\n",
        "For example, denoising with PCA lowers the noise (the not so important data) and thus thanks to some linear combinations brings out only the most important data with is called principal components. Then we are working only with the relevant data, which brings down the computational cost and the time consumed respectively. Also, the CPCV method was very useful which helps with cross validating time-series data and thus improves the learning of the dependencies in the data itself.\n",
        "This is how better mean returns were achieved while having lower volatility and lower risk-adjusted returns.\n",
        "Considering all the results above and during this experiment we can conclude that: YES, it does worth the additional complexity and effort because over all we are getting much better results and this is the final goal. It would not be worth it if we had not achieved these improvements but since we did so the final conclusion is: Yes, it all worth it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pukr2cgiTXUF"
      },
      "source": [
        "**CONCLUSION**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, for better portfolio management, we have selected the portfolio with the best risk-reward relationship and done improvements like denoising, clustering and backtesting. We have found values for different metrics like mean return, standard deviation, Sharpe Ratio, Sortino Ratio, Value at Risk (VaR), Maximum Drawdown, and Expected Shortfall (CVaR) to find the performance of the model due to these improvements. The experiment of combined improvements was also successful which resulted in better mean returns, lower standard deviation and better risk-adjusted returns.\n"
      ],
      "metadata": {
        "id": "g_sbzDXpVOC1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zqvbWcPW_5p"
      },
      "source": [
        "**REFERENCES**:\n",
        "1. Timm, Bjarne. \"Utilizing Machine Learning to Address Noise in Covariance and Correlation Matrices: An Application and Modification of Enhanced Portfolio Optimisation.\" Copenhagen Business School, Master's thesis, 2021. https://research.cbs.dk/en/studentProjects/utilizing-machine-learning-to-address-noise-in-covariance-and-cor.\n",
        "2. ChunYu Zhang, Qiujun Lan, Xiaoting Mi, Zhongding Zhou, Chaoqun Ma, Xianhua Mi. “A denoising method based on the nonlinear relationship between the target variable and input features.” ScienceDirect. Vol. 218, 2023. https://doi.org/10.1016/j.eswa.2023.119585.\n",
        "3. “Benefits Of Backtesting For Improving Performance.” FasterCapital. 20 Apr, 2024. https://fastercapital.com/topics/benefits-of-backtesting-for-improving-performance.html.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}